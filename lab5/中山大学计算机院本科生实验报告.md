中山大学计算机院本科生实验报告

#### **（2021 学年秋季学期）**

课程名称：高性能计算程序设计基础                   **批改人：**

| 实验  | **Lab5**                                          | 专业（方向） | **信息与计算科学** |
| ----- | ------------------------------------------------- | ------------ | ------------------ |
| 学号  | **18323004**                                      | 姓名         | **曾比**           |
| Email | **[2818097988@qq.com](mailto:2818097988@qq.com)** | 完成日期     |                    |

### 实验目的

1. 通过实验4构造的基于Pthreads的parallel_for函数替换fft_serial应用中的某些计算量较大的“for循环” ,实现for循环分解、分配和线程并行执行。

2. 将heated_plate_openmp应用改造成基于MPI的进程并行应用。Bonus:使用MPI_Pack/MPI_Unpack，或MPI_Type_create_struct实现数据重组后的消息传递。

3. 1）不同问题规模的并行化fft应用并行执行时间对比，其中问题规模定义为N变化范围2，4，6，8，16，32，64，128，……， 2097152；并行规模为1，2，4，8 进/线程。

   2）内存消耗对比，内存消耗采用 “valgrind massif”工具采集，注意命令valgrind命令中增加--stacks=yes 参数采集程序运行栈内内存消耗。Valgrind massif输出日志（massif.out.*pid*）经过ms_print打印后示例如下图，其中x轴为程序运行时间，y轴为内存消耗量：

### 实验过程和核心代码

1. 通过实验4构造的基于Pthreads的parallel_for函数替换fft_serial应用中的某些计算量较大的“for循环” ,实现for循环分解、分配和线程并行执行。

通过学习可以了解到，整个 fft 算法时间复杂度是 $O(nlogn)$ 的，其中 $O(logn)$ 的时间复杂度有数据依赖，无法并行，但是可以针对 $O(n)$ 复杂度的地方进行并行优化。

从 `main` 函数开始观察代码，发现 `main` 函数只是针对不同的 $n$ 进行操作。而实际计算部分在函数 `fft2` 里面，而 `fft2` 函数中有这样一段代码。

```c++
m = ( int ) ( log ( ( double ) n ) / log ( 1.99 ) );
   mj   = 1;
/*
  Toggling switch for work array.
*/
  tgle = 1;
  step ( n, mj, &x[0*2+0], &x[(n/2)*2+0], &y[0*2+0], &y[mj*2+0], w, sgn );

  if ( n == 2 )
  {
    return;
  }

  for ( j = 0; j < m - 2; j++ )
  {
    mj = mj * 2;
    if ( tgle )
    {
      step ( n, mj, &y[0*2+0], &y[(n/2)*2+0], &x[0*2+0], &x[mj*2+0], w, sgn );
      tgle = 0;
    }
    else
    {
      step ( n, mj, &x[0*2+0], &x[(n/2)*2+0], &y[0*2+0], &y[mj*2+0], w, sgn );
      tgle = 1;
    }
  }
```

变量 `m` 是 $O(logn)$ 级别大小，而且循环中也有数据依赖，无法并行。因此进一步观察 `step` 函数，下面是 `step` 函数的关键计算代码。

```c++
for ( j = 0; j < lj; j++ )
{
    jw = j * mj;
    ja  = jw;
    jb  = ja;
    jc  = j * mj2;
    jd  = jc;

    wjw[0] = w[jw*2+0]; 
    wjw[1] = w[jw*2+1];

    if ( sgn < 0.0 ) 
    {
        wjw[1] = - wjw[1];
    }

    for ( k = 0; k < mj; k++ )
    {
        c[(jc+k)*2+0] = a[(ja+k)*2+0] + b[(jb+k)*2+0];
        c[(jc+k)*2+1] = a[(ja+k)*2+1] + b[(jb+k)*2+1];

        ambr = a[(ja+k)*2+0] - b[(jb+k)*2+0];
        ambu = a[(ja+k)*2+1] - b[(jb+k)*2+1];

        d[(jd+k)*2+0] = wjw[0] * ambr - wjw[1] * ambu;
        d[(jd+k)*2+1] = wjw[1] * ambr + wjw[0] * ambu;
    }
}
```

没有我们不愿意看到的数据依赖，这就是我们期望能并行计算的代码。

因为需要使用已经实现过的 `parallel_for` 进行并行，在改写程序之前，首先说明相对于 `lab5` 对 `parallel_for` 的一些改动。

```c++
void parallel_for(int start, int end, int increment, void *(*function)(void *), void *arg, int num_threads) {
    // 1. get the number of work
    // 首先将 end 对齐，end = \lceil \frac{end}{increament} \rceil * increment
    end = (end + increment - 1) / increment * increment;
    // 然后计算 task 数量
    int tasks = (end - start) / increment;
    // 2. partition tasks
    // 循环分配，因此 increment 变为 increment * num_threads
    pthread_t worker[num_threads];
    struct for_index idx[num_threads];
    for(int i = 0; i < num_threads; ++ i) {
        idx[i].thread_id = i;
        idx[i].thread_num = num_threads;
        idx[i].arg = arg;
        idx[i].start = start + increment * i;
        idx[i].end = end;
        idx[i].increment = increment * num_threads;
        if(i > 0) {
            pthread_create(&worker[i], NULL, function, idx + i);
        }
    }
    function(idx);
    for(int i = 1; i < num_threads; ++ i)
        pthread_join(worker[i], NULL);
}
```

​	注意到主线程也可以参与计算，因此只需要分离出 `num_threads - 1` 个线程即可，主线程可以在分配完任务后参与计算，这样可以少启动一个线程，减小创建线程\销毁线程开销。

​	然后开始定义启动线程的 `solve` 函数

```c++
void* solver(void *args) {
  struct for_index *data = (struct for_index*) args;
  struct for_arg arg = *(struct for_arg*) data->arg;
  for(int i = data->start; i < data->end; i += data->increment)
  {
    arg.jw = arg.j * arg.mj;
    arg.ja  = arg.jw;
    arg.jb  = arg.ja;
    arg.jc  = arg.j * arg.mj2;
    arg.jd  = arg.jc;

    arg.wjw[0] = arg.w[arg.jw*2+0]; 
    arg.wjw[1] = arg.w[arg.jw*2+1];

    if ( arg.sgn < 0.0 ) 
    {
      arg.wjw[1] = - arg.wjw[1];
    }

    for ( arg.k = 0; arg.k < arg.mj; arg.k++ )
    {
      arg.c[(arg.jc+arg.k)*2+0] = arg.a[(arg.ja+arg.k)*2+0] + arg.b[(arg.jb+arg.k)*2+0];
      arg.c[(arg.jc+arg.k)*2+1] = arg.a[(arg.ja+arg.k)*2+1] + arg.b[(arg.jb+arg.k)*2+1];

      arg.ambr = arg.a[(arg.ja+arg.k)*2+0] - arg.b[(arg.jb+arg.k)*2+0];
      arg.ambu = arg.a[(arg.ja+arg.k)*2+1] - arg.b[(arg.jb+arg.k)*2+1];

      arg.d[(arg.jd+arg.k)*2+0] = arg.wjw[0] * arg.ambr - arg.wjw[1] * arg.ambu;
      arg.d[(arg.jd+arg.k)*2+1] = arg.wjw[1] * arg.ambr + arg.wjw[0] * arg.ambu;
    }
  }
  return NULL;
}
```

`step` 函数如下

```c++
double ambr;
double ambu;
int j;
int ja;
int jb;
int jc;
int jd;
int jw;
int k;
int lj;
int mj2;
double wjw[2];

mj2 = 2 * mj;
lj  = n / mj2;
struct for_arg arg;
arg.lj = lj;
arg.mj2 = mj2;
arg.n = n;
arg.mj = mj;
arg.a = a;
arg.b = b;
arg.c = c;
arg.d = d;
arg.w = w;
arg.sgn = sgn;
parallel_for(0, lj, 1, solver, (void*)&arg, 4);
return;
```

​	同样，开启多线程后，原本的 `clock` 函数已经无法正常计时，这里使用 `c++11` 标准的 `std::chrono` 进行计时，`double get_current_time` 进行如下改写。

```c++
double get_current_time() {
  auto current = std::chrono::system_clock().now();
  return std::chrono::time_point_cast<std::chrono::duration<double>>(current).time_since_epoch().count();
}
```

​	因此，源代码需要从 `.c` 变为 `.cpp` 。结果在改变后，编译时出现了链接错误。在使用 `nm` 指令核对符号后，发现了问题是因为原本的 `parallel_for.c` 是 `.c` 的代码，使用 `gcc` 编译的符号和 `g++` 编译的符号不一致，导致 `fft_serial.cpp` 链接的过程中无法从 `libparallel_for.so` 文件中找到对应的符号。

​	因此需要改变 `parallel_for.h` 的写法，加入 `extern "C"` 使 `parallel_for` 函数强制按照 `C` 的风格编译。

```c++
#ifndef PARALLEL_FOR_H

#define PARALLEL_FOR_H

#ifdef __cplusplus
extern "C"  {
#endif
struct for_index {
    void *arg;
    int start;
    int end;
    int increment;
    int thread_id;
    int thread_num;
};
void parallel_for(int start, int end, int increment, void *(*function)(void *), void *arg, int num_threads);
#ifdef __cplusplus
}
#endif

#endif
```

**同时，运行程序前一定需要将环境变量 LD_LIBRARY_PATH 修改为 `libparallel_for.so` 的位置  **

```bash
export LD_LIBRARY_PATH=$PATH
```



2. 将heated_plate_openmp应用改造成基于MPI的进程并行应用。Bonus:使用MPI_Pack/MPI_Unpack，或MPI_Type_create_struct实现数据重组后的消息传递。

​	针对简单循环，可以使用循环分配的方式，容易实现。

```c++
memset(w, 0 ,sizeof w);
for ( i = myrank + 1; i < M - 1; i+= size )
{
    w[i][0] = 100.0;
}
for ( i = myrank + 1; i < M - 1; i+=size )
{
    w[i][N-1] = 100.0;
}
for ( j = myrank; j < N; j+=size )
{
    w[M-1][j] = 100.0;
}
for ( j = myrank; j < N; j+=size )
{
    w[0][j] = 0.0;
}
MPI_Allreduce(MPI_IN_PLACE, w, N * M, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
for ( i = myrank + 1; i < M - 1; i += size)
{
    mean = mean + w[i][0] + w[i][N-1];
}
for ( j = myrank; j < N; j += size )
{
    mean = mean + w[M-1][j] + w[0][j];
}
MPI_Allreduce(MPI_IN_PLACE, &mean, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
mean = mean / ( double ) ( 2 * M + 2 * N - 4 );
```

​	上述代码不是重点，因为整个代码的复杂度瓶颈在后面的迭代计算上面，提高迭代计算的效率才能提高整体并行效率。在讲解如何提高迭代计算效率之前，介绍准备工作。

1. 定义函数 `getLocak`

```c++
int getLocal(int tot, int myrank, int size) {
  int localsize = tot / size;
  int x = tot - tot / size * size;
  int mn = size - x;
  if(myrank >= mn)
    ++localsize;
  return localsize;
}
```

​	`getLocal` 函数是专门用来计算，`n` 个任务，平均分配给 `m` 个进程，第 `i` 个进程分配到多少个任务这个问题。为了让任务分配数量尽可能平均，我们可以得到一个结论，`m` 个进程中，至少存在一种分配方式使得执行任务数量差最多为 $1$ ，这个结论很容易证明。证明如下。

​	如果 $m \mid n$ 那么每个进程都分配 $n/m$ 个任务；否则令前 $x$ 个进程分配 $\lfloor n/m \rfloor$ 个任务，后面 $m-x$ 个进程分配 $\lfloor n/m \rfloor + 1$ 个任务，可以得到方程。
$$
\lfloor n/m \rfloor x + (m-x)(\lfloor n/m \rfloor + 1)=n
$$
​	也就是可以求出 $x=m(\lfloor n/m \rfloor + 1)- n=m ((n-n\bmod m) /m + 1) - n =m-n\bmod m>0$ ，使用这种方式，也就是 `getLocal` 的算法，可以让任务尽可能平均分配到进程上。

2. 准备工作。

```c++
diff = epsilon;
int startPos[size], local[size];
for(i = 0; i < size; ++ i)
    local[i] = getLocal(M, i, size);
startPos[0] = 0;
for(i = 1; i < size; ++ i)
    startPos[i] = startPos[i - 1] + local[i - 1];
int recv[size], disp[size];
for(int i = 0; i < size; ++ i) {
    recv[i] = local[i] * M;
    disp[i] = startPos[i] * M;
}
```

​	其中 `Local` 是每个进程的任务量（只会根据 $M$ 分配任务），`startPos` 是每个进程的起始地址，是 `Local` 的前缀和，`recv` 和 `disp` 是 `Local` 和 `startPos` 乘 $M$ 

**迭代部分**

按照前面描述的任务分配方式进行任务分配。

1. 保存旧数据

```c++
for(i = max(0, startPos[myrank] - 1); i < min(M , startPos[myrank] + local[myrank] + 1); ++ i) {
      for(j = 0; j < N; ++ j) {
        u[i][j] = w[i][j];
      }
    }
```

​	因为第 $i$ 行会需要访问到第 $i+1$ 行和 $i-1$ 行的数据，因此让任务复制区域向上下拓宽一个单位。

2. 迭代计算

```c++
for(i = startPos[myrank]; i < startPos[myrank] + local[myrank]; ++ i) {
    if(i == 0 || i == M - 1) continue;
    for(j = 1; j < N - 1; ++ j) {
        w[i][j] = ( u[i-1][j] + u[i+1][j] + u[i][j-1] + u[i][j+1] ) / 4.0;
    }
}
MPI_Allgatherv(MPI_IN_PLACE, N * M, MPI_DOUBLE, w, recv, disp , MPI_DOUBLE, MPI_COMM_WORLD);
```

​	每个任务各司其职。

3. 计算误差

```c++
diff = 0.0;
for(i = startPos[myrank]; i < startPos[myrank] + local[myrank]; ++ i) {
    for ( j = 1; j < N - 1; j++ )
    {
        if ( diff < fabs ( w[i][j] - u[i][j] ) )
        {
            diff = fabs ( w[i][j] - u[i][j] );
        }
    }
}
MPI_Allreduce(MPI_IN_PLACE, &diff, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);
```

​	自此，全部优化完成。



3. 1）不同问题规模的并行化fft应用并行执行时间对比，其中问题规模定义为N变化范围2，4，6，8，16，32，64，128，……， 2097152；并行规模为1，2，4，8 进/线程。

   2）内存消耗对比，内存消耗采用 “valgrind massif”工具采集，注意命令valgrind命令中增加--stacks=yes 参数采集程序运行栈内内存消耗。Valgrind massif输出日志（massif.out.*pid*）经过ms_print打印后示例如下图，其中x轴为程序运行时间，y轴为内存消耗量：

   使用如下命令安装

   ```bash
   sudo apt-get install -y valgrind massif-visualizer
   ```

   这里的其他结论在实验结果部分中。

### 实验结果

1. 实现结果如下图



<img src="D:\课件\大三\高性能计算\lab\lab5\pic\1.png" alt="1" style="zoom:67%;" />

2. 实验结果如下图

​	<img src="D:\课件\大三\高性能计算\lab\lab5\pic\2.png" alt="2" style="zoom:67%;" />

​	实验发现与 `openmp` 版本的迭代次数不同，`openmp` 版本开启 $16$ 个线程最终只用迭代 $11449$ 次。经过研究，应该是多线程的 `openmp` 导致的某种精度问题，具体是什么原因有待考究，随着 `openmp` 线程的增加，精度丢失越来越验证，迭代次数会越来越少。而 `mpi` 则完全没有这个问题。

​	**补充说明**：最后发现是以下代码少抄了一句话

```c++
# pragma omp parallel shared ( diff, u, w ) private ( i, j, my_diff ) num_threads(16)
    {
      my_diff = 0; //少写了这句话
# pragma omp for
      for ( i = 1; i < M - 1; i++ )
      {
        for ( j = 1; j < N - 1; j++ )
        {
          if ( my_diff < fabs ( w[i][j] - u[i][j] ) )
          {
            my_diff = fabs ( w[i][j] - u[i][j] );
          }
        }
      }
# pragma omp critical
      {
        if ( diff < my_diff )
        {
          diff = my_diff;
        }
      }
    }
```

​	因为 `private` 里面的变量是未定义的，使用未定义变量是未定义行为，因此导致了上述错误。同时我发现，类似于这种错误写法，`gcc` 不会报 `warning`，我觉得这可能是 `gcc` 可以改进的一个点。

```c++
int x;
if( x < 1) x = 1;
```

3. 1）

​	下表中，$N = 1$ 时，使用原本的串行结果，其他都使用 `parallel_for` 优化后的结果。其中，表格数据为 `MFLOPS` 

| N \ 进程数 | 1           | 2           | 4           | 8           |
| ---------- | ----------- | ----------- | ----------- | ----------- |
| 2          | 328.064451  | 0.359865    | 0.201210    | 0.092383    |
| 4          | 533.626463  | 0.719348    | 0.401204    | 0.184166    |
| 8          | 755.333503  | 1.410869    | 0.808309    | 0.370586    |
| 16         | 802.233812  | 2.780675    | 1.635259    | 0.738608    |
| 32         | 879.539502  | 5.659685    | 3.257410    | 1.479447    |
| 64         | 884.611817  | 11.127250   | 6.520198    | 2.961703    |
| 128        | 978.900363  | 21.912907   | 13.077544   | 5.915354    |
| 256        | 969.299773  | 44.421192   | 25.850320   | 11.907180   |
| 512        | 985.938521  | 87.307914   | 50.556387   | 23.995392   |
| 1024       | 1040.876159 | 164.778470  | 101.614663  | 47.655776   |
| 2048       | 1098.789224 | 311.384386  | 202.058192  | 96.212935   |
| 4096       | 1066.217218 | 513.363157  | 377.105053  | 188.514078  |
| 8192       | 1126.435161 | 797.764996  | 668.957945  | 365.107853  |
| 16384      | 1089.846248 | 1032.167145 | 1064.840424 | 662.146703  |
| 32768      | 1143.939175 | 1314.375165 | 1620.908035 | 1156.030741 |
| 65536      | 1115.075291 | 1465.277980 | 1977.289959 | 1799.962557 |
| 131072     | 1164.896588 | 1574.143746 | 2322.123099 | 2637.313778 |
| 262144     | 1142.211293 | 1597.443705 | 2441.761477 | 3151.416267 |
| 524288     | 1160.531133 | 1723.599958 | 2214.442770 | 3338.989376 |
| 1048576    | 1127.137775 | 1707.193587 | 2236.922312 | 2523.023644 |

表中数据容易得到结论，随着线程增多，较小的 $N$ 执行时间会变得更长，而较大的 $N$ 执行时间会变得更快。原因是创建线程和销毁线程占用了大时间。而一次 `fft` 的实现过程中，会出现很多次 `N` 比较小的运算，因此可以考虑创建一个阈值，当 $2 \le N \le 16384$ 的时候，选择串行，当 $16384 < N \le 65536$ 的时候，选择 $4$ 线程并行，当 $N > 65536$ 的时候，选择 $8$ 线程并行。

2)

##### 串行

```
    MB
56.09^                                              ::@@:::::@::  @:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                                            #:::@ :::::@::::@:::::@:::::
     |                               ::::::::@::::#:::@ :::::@::::@:::::@:::::
     |                               ::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                               ::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                               ::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                               ::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                        ::@::@:::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                        ::@::@:::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                        ::@::@:::::::::@::: #:::@ :::::@::::@:::::@:::::
     |                      :@::@::@:::::::::@::: #:::@ :::::@::::@:::::@:::::
     |              ::::::@@:@::@::@:::::::::@::: #:::@ :::::@::::@:::::@:::::
   0 +----------------------------------------------------------------------->Gi
     0                                                                   17.44

```

##### 2 线程

```c++
    MB
100.1^                                                                       :
     |                                            #::::@::::::@:::::@::::::@::
     |                                            #::::@::::::@:::::@::::::@::
     |                                            #::::@::::::@:::::@::::::@::
     |                                            #::::@::::::@:::::@::::::@::
     |                                            #::::@::::::@:::::@::::::@::
     |                               ::::::::::::@#::::@::::::@:::::@::::::@::
     |                               ::::::::: : @#::::@::::::@:::::@::::::@::
     |                               ::::::::: : @#::::@::::::@:::::@::::::@::
     |                        @::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |                      ::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |            :::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |    :::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |  :::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |  :::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     |  :::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     | ::::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     | ::::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     | ::::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
     | ::::::@:@@::::@::::::::@::::::::::::::: : @#::::@::::::@:::::@::::::@::
   0 +----------------------------------------------------------------------->Gi
     0                                                                   17.93
```

##### 4 线程

```
    MB
188.1^                                                                       :
     |                                             #:::::::@::::::@:::::::@:::
     |                                             #:::::::@::::::@:::::::@:::
     |                                 @::::::::::@#:::::::@::::::@:::::::@:::
     |                                 @::::::::::@#:::::::@::::::@:::::::@:::
     |                           ::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |            :::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |      :::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |     ::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |    :::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |    :::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |   ::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |   ::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |   ::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |  @::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |  @::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     |  @::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     | :@::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     | :@::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
     | :@::::::::::::::@:::::@:::::::::@::::::::::@#:::::::@::::::@:::::::@:::
   0 +----------------------------------------------------------------------->Gi
     0                                                                   18.53
```

##### 8 线程

```
    MB
364.1^                                                                       :
     |                                               #:::::::::@:::::::@::::::
     |                                     ::::::::@:#:::::::::@:::::::@::::::
     |                     ::::::::::::::::::::::::@:#:::::::::@:::::::@::::::
     |            @:::@::::: :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |          :@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |          :@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |         @:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |        :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |        :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |      @@:@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |     :@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |     @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |     @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |     @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |   ::@@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |   : @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     | ::: @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     | : : @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
     |:: : @@ :@:@@: :@::: : :: :::: ::::: ::::::::@:#:::::::::@:::::::@::::::
   0 +----------------------------------------------------------------------->Gi
     0                                                                   20.28
```

​	运行内存随着启动线程增加线性增长

### 实验感想 

​	这次实验主要花费时间在实验一上，对实验一的并行反而降速的现象研究了一段时间。最后认定是因为创建线程\销毁线程的时间导致了并行在 $N$ 较小的时候效率更低。也就是说无脑并行优化是不实际的，需要通过实验找，针对不同的数据找到不同程度的优化，这样才可以达到最好的优化效果。 

​	针对于需要共享内存的代码还是使用多线程，实验二中使用 `mpi` 终究在运行数据上还是比不过 `openmp` ，主要原因是 `mpi` 需要大量的消息传递保证数据同步。前面的 lab 都不会有太多消息传递，无法体现出消息传递带来的严重效率问题，这次实验中有至少 $16000 \times2$ 次的消息传递，明显效率低了不少。

​	并行除了会加速运行时间，还会导致使用内存大幅度增长，对于使用内存比较大的程序，用并行的时候一定得注意内存占用大小。

