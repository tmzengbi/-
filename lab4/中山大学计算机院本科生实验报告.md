## 中山大学计算机院本科生实验报告

#### **（2021 学年秋季学期）**

课程名称：高性能计算程序设计基础                   **批改人：**

| 实验  | **Lab4**                                          | 专业（方向） | **信息与计算科学** |
| ----- | ------------------------------------------------- | ------------ | ------------------ |
| 学号  | **18323004**                                      | 姓名         | **曾比**           |
| Email | **[2818097988@qq.com](mailto:2818097988@qq.com)** | 完成日期     |                    |

### 实验目的

1. 通过OpenMP实现通用矩阵乘法
2. 基于OpenMP的通用矩阵乘法优化
3. 构造基于Pthreads的并行for循环分解、分配和执行机制。

### 实验过程和核心代码

#### 实验环境

```
windows10 使用 WSL2 的 Ubuntu20.04 子系统
CPU 为 AMD Ryzen 7 5800H with Radeon Graphics，8 核心 16 线程。
编译环境为 gcc 9.4.0
```



#### 通过OpenMP实现通用矩阵乘法

通过 `OpenMP` 实现通用矩阵乘法

矩阵乘法部分仍然使用 `lab1` 得到的最优的方法——针对 `cache` 进行优化，于是核心计算代码如下。其中 `omp_get_num_threads `， `omp_get_thread_num` 分别是得到线程数量和当前线程的 `id`。

```c++
int N,M,K;
float *A, *B, *C;
// A is N * M
// B is M * K
// C is N * K
void solve() {
    #define idA (i * M + k) 
    #define idB (k * K + j)
    #define idC (i * K + j)
    int size = omp_get_num_threads();
    int rank = omp_get_thread_num();
    for(int i = rank; i < N; i += size)
        for(int k = 0; k < M; ++ k) {
            float tmp = A[idA];
            for(int j = 0; j < K; ++ j)
                C[idC] += tmp * B[idB];
        }
    #undef idA
    #undef idB
    #undef idC
}
```

现在就是要如何设计 `omp` 的并行。

1. 通过参数得到参与并行的线程数量。

```c++
if(argc != 2) {
    puts("number of thread needed");
    exit(-1);
}
int size = strtol(argv[1], NULL, 10);
```

2. 读取文件里面的矩阵。

```c++
if((fp = fopen("a.in", "r")) == NULL) {
    perror("cannot open file");
    exit(-1);
}
fscanf(fp, "%d%d%d", &N, &M, &K);
A = (float*) malloc (sizeof(float) * N * M);
B = (float*) malloc (sizeof(float) * M * K);
C = (float*) malloc (sizeof(float) * N * K);
memset(C, 0, sizeof(float) * N * K);
for(int i = 0; i < N * M; ++ i)
    fscanf(fp, "%f", A + i);
for(int i = 0; i < M * K; ++ i)
    fscanf(fp, "%f", B + i);
```

3. 执行并行操作。使用最简单的 `omp` 并行方法，使用默认的任务分配方式，执行并行操作。

```c++
#pragma omp parallel num_threads(size)
    solve();
```

4. 计算时间，因为 `clock` 函数是按照 `cpu` 时钟来计算的时间，因此并行条件下会导致时间不准确。这里使用了 `c++11` 标准的 `std::chrono` 的 `steady_clock` 来计算执行时间。代码如下。

```c++
auto start = std::chrono::steady_clock::now();
#pragma omp parallel num_threads(size)
solve();
auto duration = std::chrono::steady_clock::now() - start;
double lasting = std::chrono::duration_cast<std::chrono::duration<double>>(duration).count();
printf("%.10lf\n", lasting);
```

我选取了矩阵大小 $N=256$ , $512$ , $1024$ , $2048$  ，线程个数 $S=1,2,3\dots8$ 。每次都随机生成 $2$ 个 $N\times N$  的方阵，记录运算时间。计算效率的测试在实验结果中呈现。



#### 基于OpenMP的通用矩阵乘法优化

**任务：分别采用OpenMP的默认任务调度机制、静态调度schedule(static, 1)和动态调度schedule(dynamic,1)，调度#pragma omp for的并行任务，并比较其性能。**

静态调度：在执行前就为每个线程分配任务，代码如下。

```c++
#pragma omp parallel for num_threads(size) \
    default(none) shared(A, B, C, N, M, K) \
        schedule(static,1)
    for(int i = 0; i < N; ++ i)
        for(int k = 0; k < M; ++ k) {
            float tmp = A[idA];
            for(int j = 0; j < K; ++ j)
                C[idC] += tmp * B[idB];
        }
```

动态调度：执行时才为每个线程分配任务，代码如下。

```c++
#pragma omp parallel for num_threads(size) \
    default(none) shared(A, B, C, N, M, K) \
        schedule(dynamic,1)
    for(int i = 0; i < N; ++ i)
        for(int k = 0; k < M; ++ k) {
            float tmp = A[idA];
            for(int j = 0; j < K; ++ j)
                C[idC] += tmp * B[idB];
        }
```

同样选取了矩阵大小 $N=256$ , $512$ , $1024$ , $2048$  ，线程个数 $S=1,2,3\dots8$ 。每次都随机生成 $2$ 个 $N\times N$  的方阵，记录运算时间。计算效率的测试在实验结果中呈现。



#### 构造基于Pthreads的并行for循环分解、分配和执行机制。

这个是我耗时最多的部分，主要难点是如何设计 `parallel_for` 函数。

首先定义了 for_index 结构体。这个结构体表示一个任务的所有属性。目前只有如下属性，如果有其他属性的需要也可以随时加上。

```c++
struct for_index {
    void *arg;
    int start;
    int end;
    int increment;
    int thread_id;
    int thread_num;
};
```

关于这个函数的设计我有以下几个想法。

1. 动态调度。开启一个线程池，然后在 `parallel_for` 里面分配任务。
2. 静态调度。在建立线程前就分配好任务。

考虑编程难度，我选择了静态调度。

首先对于这样一个函数 `void parallel_for(int start, int end, int increment, void *(*function)(void *), void *arg, int num_threads)` 。

1. 我可以考虑先将 `end` 对其，即让 $end = start + nd$ ，也就是 $end = \lceil\frac{end}{d}\rceil d$，这样可以让代码更容易调试，也能让与 $end$ 相关的计算更加简便。

```c++
end = (end + increment - 1) / increment * increment;
```

2. 然后计算任务数量。

```c++
int tasks = (end - start) / increment;
```

3. 最后进行任务分配。

​	关于任务分配有两种方式

- 分块分配，即 id 为 $1,2\dots k$ 的任务分到第一个线程，id 为 $k+1\dots 2k$ 的任务分到第二个线程，$\dots$
- 循环分配，即 id 为 $1,k+1 \dots$ 的任务分配到第一个线程，id 为 $2,k+2\dots$ 的任务分配到第二个线程，$\dots$ ，简单的说就是 id 为 $x$ 的任务分配到第 $x \bmod k + 1$ 个线程。

这里为了简单实现就使用了循环分配，即将 tasks 个任务分配到 num_threads 个线程中。因为任务都是由一个三元组表示 $(start,end,increment)$ ，循环分配的话我可以简单地将 $increment$ 乘上 num_threads ，来达到第 $i$ 个任务下一个任务是第 $i+k$ 个任务的效果。实现简单。代码如下。

```c++
pthread_t worker[num_threads];
struct for_index idx[num_threads];
for(int i = 0; i < num_threads; ++ i) {
    idx[i].thread_id = i;
    idx[i].thread_num = num_threads;
    idx[i].arg = arg;
    idx[i].start = start + increment * i;
    idx[i].end = end;
    idx[i].increment = increment * num_threads;
    pthread_create(&worker[i], NULL, function, idx + i);
}
for(int i = 0; i < num_threads; ++ i)
    pthread_join(worker[i], NULL);
```



### 实验结果

首先验证上述代码的正确性。随机造一个简单矩阵乘法的数据。

```
2 * 3 矩阵 A     3 * 4 矩阵 B
1 5 2			1 8 6 3
6 2 1 			3 9 0 2 
				8 4 2 1
```

得到结果为正确结果。

```
32.000000 61.000000 10.000000 15.000000 
20.000000 70.000000 38.000000 23.000000
```

关于实验，为了避免误差，每个变量的实验都会进行 $5$ 次，实验结果取平均值。

为了避免重复的浪费时间的测试，启用全自动脚本测试，脚本编写如下。（其中 `generate.py` 是用来生成随机矩阵的 `python` 脚本）

```shell
# 运行脚本
for file in $@; do
    echo "start solve $file"
    g++ $file -o test -fopenmp
    for size in 256 512 1024 2048; do
        echo "generate $size X $size matrix"
        python3 generate.py $size # generate ramdom data
        for ((p=1; p<=8; p++)); do
            echo "use $p thread "
            for ((i=1; i<=5; i++)) ;do 
                # mpiexec -n $p ./test
                ./test $p
                sleep 0.1
            done        
        done
    done
done
```

最后，以下实验结果单位全部为秒。

#### 通过OpenMP实现通用矩阵乘法

实验结果如下：

| 线程数量\矩阵规模 | 256          | 512          | 1024         | 2048          |
| ----------------- | ------------ | ------------ | ------------ | ------------- |
| 1                 | 0.0463636314 | 0.3723276554 | 2.9630691692 | 23.3618962744 |
| 2                 | 0.0247239422 | 0.1951115662 | 1.5112219788 | 12.2038305366 |
| 3                 | 0.0177121452 | 0.1435125200 | 1.1552103010 | 8.9726392306  |
| 4                 | 0.0125172860 | 0.1162231842 | 0.9170912972 | 7.1650334458  |
| 5                 | 0.0100335880 | 0.1005107826 | 0.7554094320 | 5.9805679908  |
| 6                 | 0.0086403482 | 0.0832864418 | 0.6725643460 | 5.1937997318  |
| 7                 | 0.0085286852 | 0.0751741404 | 0.6302735906 | 4.6718775410  |
| 8                 | 0.0066528742 | 0.0635053938 | 0.5467724940 | 4.1965468632  |



#### 基于OpenMP的通用矩阵乘法优化

实验结果如下

**静态调度：**

| 线程数量\矩阵规模 | 256          | 512          | 1024         | 2048          |
| ----------------- | ------------ | ------------ | ------------ | ------------- |
| 1                 | 0.0448463768 | 0.3618797094 | 2.9043968248 | 23.2082924112 |
| 2                 | 0.0234312304 | 0.1858399748 | 1.5105331764 | 12.2676904056 |
| 3                 | 0.0161423108 | 0.1301707418 | 1.1783944250 | 8.6575005972  |
| 4                 | 0.0123205766 | 0.1047605472 | 0.9230309730 | 6.9724803304  |
| 5                 | 0.0099156744 | 0.0902145310 | 0.8115989562 | 5.9137187680  |
| 6                 | 0.0084493102 | 0.0765758542 | 0.6534010446 | 5.1764485866  |
| 7                 | 0.0075062696 | 0.0691252516 | 0.6167983540 | 4.5178127994  |
| 8                 | 0.0071767138 | 0.0633212242 | 0.5754902816 | 4.2359186090  |

**动态调度：**

| 线程数量\矩阵规模 | 256          | 512          | 1024         | 2048          |
| ----------------- | ------------ | ------------ | ------------ | ------------- |
| 1                 | 0.0460485402 | 0.3640678652 | 2.8905436588 | 23.2275947134 |
| 2                 | 0.0241521844 | 0.1896586174 | 1.4991546776 | 12.2178014620 |
| 3                 | 0.0162575718 | 0.1333852912 | 1.0763352596 | 8.6704086976  |
| 4                 | 0.0125462098 | 0.0963357724 | 0.8250996018 | 6.9234466178  |
| 5                 | 0.0103826684 | 0.0796427250 | 0.7221352424 | 5.7477595388  |
| 6                 | 0.0085225674 | 0.0691723408 | 0.5881068350 | 5.0299142862  |
| 7                 | 0.0074904274 | 0.0629328292 | 0.5383514976 | 4.4397169370  |
| 8                 | 0.0066721702 | 0.0534499352 | 0.4830647336 | 3.9658321194  |

观察实验数据，似乎动态调度会比静态调度快一些，在线程数量多的时候尤为明显。我认为原因是静态调度往往会出现有些线程先完成任务，有些线程后完成任务的情况。而动态调度原理类似于流水线，流水线级数越多的情况下，时间会得到越充分的利用，因此动态调度会更快。

​	为了对猜想进行确认，我生成 $4096\times 4096$ 的矩阵进行进一步测试，得到如下结果

| 线程数量\调度方式 | 静态调度       | 动态调度       |
| ----------------- | -------------- | -------------- |
| 1                 | 186.2182568876 | 187.6927670134 |
| 2                 | 99.3871540784  | 99.7387022802  |
| 3                 | 69.7040035620  | 70.9010913596  |
| 4                 | 55.6427358984  | 57.0093248096  |
| 5                 | 47.6581790464  | 47.0210138910  |
| 6                 | 42.3650071562  | 41.3099529052  |
| 7                 | 37.1361890582  | 37.0767508474  |
| 8                 | 33.5049325148  | 33.2306350190  |

​	可以看到，线程数量比较少的时候，动态调度甚至慢于静态调度，这是由于动态调度额外的开销导致的，而线程数量达到一个阈值后，动态调度速度会快于静态调度。

#### 构造基于Pthreads的并行for循环分解、分配和执行机制。

目前要设计一个 `test.c` 函数来进行测试。

1. 首先实现 `test.c` 中的 `solve` 函数

```c++
void *solve(void *arg) {
    struct for_index *idx = (struct for_index*) arg;
    int start = idx->start;
    int end = idx->end;
    int increment = idx->increment;
    for(int i = start; i < end; i += increment) {
        #define idA (i * M + k) 
        #define idB (k * K + j)
        #define idC (i * K + j)
        for(int k = 0; k < M; ++ k) {
            float tmp = A[idA];
            for(int j = 0; j < K; ++ j)
                C[idC] += tmp * B[idB];
        }
        #undef idA
        #undef idB
        #undef idC
    }
    return NULL;
}
```

2. 调用 `parallel_for` 函数

```c++
parallel_for(0, N, 1, solve, NULL, size);
```

使用上面的测试数据，并启动 $4$ 个线程进行测试

```
2 * 3 矩阵 A     3 * 4 矩阵 B
1 5 2			1 8 6 3
6 2 1 			3 9 0 2 
				8 4 2 1
```

得到结果如下图

<img src="D:\课件\大三\高性能计算\lab\lab4\pic\1.png" alt="1" style="zoom:50%;" />

表示实验代码正确。

### 实验感想 

**首先说一下对上一个实验的一些新的理解。**

1. 上一个实验中，多进程 `mpi` 比 `pthread` 的多线程还要快很多，这一点让我疑惑了很久，因为实际上应该是创建进程的开销更大，而创建线程的开销更小。

​	后面和同学讨论才了解到。有一个很大的原因是在 `mpi` 实验中，当我记录开始时间时，进程已经建立好，而我记录结束时间的时候，进程还没有释放。而在 `pthread` 的实验中，当我记录开始时间时，线程还没有建立好，而当我记录结束时间时，线程资源已经通过 `pthread_join` 释放。也就是说，`pthread` 相比 `mpi` 实验多了**创建线程**和**释放线程**的时间。

2. 上一个实验中，我的加速比和效率不如同学的高。

​	和讨论后，猜想到可能出现的原因是——我使用本地的笔记本环境， CPU 因高负荷工作，温度迅速升高导致触发了降频的保护机制，而同学是用的不会降频的服务器环境。因此他的加速比和效率都比我高。



**现在是我对这个实验的感想**

​	实际上，动态调度和静态调度对比实验中，得到动态调度在线程数量多的情况下更有效率的结论不是特别有说服力，因为物理机的种种情况会导致不同程度的误差，（种种情况包括但不限于 CPU 降频，计算线程被其他线程抢占引发的上下文切换），但是多次实验全部都能得到类似的结论，我认为该结论的可信程度还是挺高的）
