## 中山大学计算机院本科生实验报告

#### **（2021 学年秋季学期）**

课程名称：高性能计算程序设计基础                   **批改人：**

| 实验  | **Lab1**                                          | 专业（方向） | **信息与计算科学** |
| ----- | ------------------------------------------------- | ------------ | ------------------ |
| 学号  | **18323004**                                      | 姓名         | **曾比**           |
| Email | **[2818097988@qq.com](mailto:2818097988@qq.com)** | 完成日期     |                    |

1. 实验目的

   实现矩阵乘法的一些优化，并与intel-mkl计算库运算时间做对比

2. 实验过程和核心代码

   实验环境：Ubuntu20.04，gcc9.3.0，阿里云 ECS 实例，2核，Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz。

   

   最近又学习了一些 c++ 的知识，计划将矩阵运算封装成一个类，在实现矩阵乘法的基础上，练习 c++ 水平。
   
   
   
   首先使用模板类封装了 Matrix 类，MKL_Matrix 类，sparseMatrix 类。具体封装细节可看代码，

​		然后封装了 solution 类，将所有解决矩阵乘法的方法封装起来。下面分别别这些方法进行介绍。

​		首先是没有任何优化，最朴素的矩阵乘法。

```c++
template<int lN, int lM /* = rN*/, int rM>
    static Matrix<lN, rM> Regular_v1(const Matrix<lN, lM> &lhs, const Matrix<lM, rM> &rhs) {
        Matrix<lN, rM> res;
        for(int i=0;i<lN;++i)
            for(int j=0;j<rM;++j)
                for(int k=0;k<lM;++k) 
                    res.data[i][j] += lhs.data[i][k] * rhs.data[k][j];
        return res;
    }
```

接下来是优化算法

#### 优化算法

##### （1）基于算法分析的方法对矩阵乘法进行优化，这里使用 Strassen 算法，Strassen 算法实现如下。

```c++
    template<int NN>
    static Matrix<NN,NN> Strassen_solve(const Matrix<NN ,NN> &lhs, const Matrix<NN ,NN> &rhs) {
        #ifdef _CHECK
        if constexpr (NN == 1) [
            return Matrix<1,1>(vector<vector<float>>({{lhs.data[0][0] * rhs.data[0][0]}}));
        ]
        #endif
        if constexpr (NN <= 64) {
            return Regular_v3(lhs,rhs);
        }
        const int MM = NN / 2;
        Matrix<MM,MM> A11, A12, A21, A22;
        Matrix<MM,MM> B11, B12, B21, B22;
        for(int i=0;i<MM;++i)
            for(int j=0;j<MM;++j) {
                A11.data[i][j] = lhs.data[i][j];
                B11.data[i][j] = rhs.data[i][j];
                A12.data[i][j] = lhs.data[i][j + MM];
                B12.data[i][j] = rhs.data[i][j + MM];
                A21.data[i][j] = lhs.data[i + MM][j];
                B21.data[i][j] = rhs.data[i + MM][j];
                A22.data[i][j] = lhs.data[i + MM][j + MM];
                B22.data[i][j] = rhs.data[i + MM][j + MM];
            }
        Matrix<MM,MM> S1 = B12 - B22;
        Matrix<MM,MM> S2 = A11 + A12;
        Matrix<MM,MM> S3 = A21 + A22;
        Matrix<MM,MM> S4 = B21 - B11;
        Matrix<MM,MM> S5 = A11 + A22;
        Matrix<MM,MM> S6 = B11 + B22;
        Matrix<MM,MM> S7 = A12 - A22;
        Matrix<MM,MM> S8 = B21 + B22;
        Matrix<MM,MM> S9 = A11 - A21;
        Matrix<MM,MM> S10 = B11 + B12;

        Matrix<MM,MM> P1 = Strassen_solve(A11, S1);
        Matrix<MM,MM> P2 = Strassen_solve(S2, B22);
        Matrix<MM,MM> P3 = Strassen_solve(S3, B11);
        Matrix<MM,MM> P4 = Strassen_solve(A22, S4);
        Matrix<MM,MM> P5 = Strassen_solve(S5,S6);
        Matrix<MM,MM> P6 = Strassen_solve(S7,S8);
        Matrix<MM,MM> P7 = Strassen_solve(S9,S10);

        Matrix<MM,MM> C11 = P5 + P4 - P2 + P6;
        Matrix<MM,MM> C12 = P1 + P2;
        Matrix<MM,MM> C21 = P3 + P4;
        Matrix<MM,MM> C22 = P5 + P1 - P3 - P7;

        Matrix<NN,NN> res;
        for(int i=0;i<MM;++i)
            for(int j=0;j<MM;++j) {
                res.data[i][j] = C11.data[i][j];
                res.data[i][j + MM] = C12.data[i][j];
                res.data[i + MM][j] = C21.data[i][j];
                res.data[i + MM][j + MM] = C22.data[i][j];
            }
        return res;
    }

template<int lN, int lM, int rN, int rM>
    static Matrix<lN, rM> Strassen(const Matrix<lN, lM> &lhs, const Matrix<rN, rM> &rhs) {
        constexpr int NN = upperBound2<lN, lM, rN, rM>::value;
        Matrix<NN, NN> lv(lhs.data), rv(rhs.data);
        return Matrix<lN, rM>(Strassen_solve(lv,rv).data);
    }
```

​	由于递归深度过深会导致大量的内存消耗，以及耗费大量的时间，因此在矩阵足够小的时候选择带优化的朴素算法。

​	**upperBound2 是模板元编程，令 $f(x)$ 是大于等于 $x$ 的 $2$ 的幂，upperBound2 用于求所有参数的最大 $f(x)$  ，详情可以看 util.hpp**  

其中 Strassen_solve 函数是 Strassen 算法的计算部分，本质是将矩阵 A 和矩阵 B 分成了四个部分，即
$$
A =\begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22}\end{bmatrix} \,\,\,\,\,\,\,
B =\begin{bmatrix} B_{11} & B_{12} \\ B_{21} & B_{22}\end{bmatrix}
$$
其中 A，B 矩阵都是 $N * N$  $N \in 2^k$ 矩阵，然后有
$$
S_1 = B_{12} - B_{22} \\
S_2 = A_{11} + A_{12} \\
S_3 = A_{21} + A_{22} \\
S_4 = B_{21} - B_{11} \\
S_5 = A_{11} + A_{22} \\
S_6 = B_{11} + B_{22} \\
S_7 = A_{12} - A_{22} \\
S_8 = B_{21} + B_{22} \\
S_9 = A_{11} - A_{21} \\
S_{10} = B_{11} + B_{12} 
$$
令
$$
P_1 = A_{11} \times S_1 \\
P_2 = S_2 \times B_{22} \\
P_3 = S_3 \times B_{11} \\
P_4 = A_{22} \times S_4 \\
P_5 = S_5 \times S_6 \\
P_6 = S_7 \times S_8 \\
P_7 = S_9 \times S_{10}
$$
然后有 
$$
C_{11} = P_5+P_4-P_2+P_6 \\
C_{12} = P_1 + P_2 \\
C_{21} = P_3 + P_4 \\
C_{22} = P_5 + P_1 - P_3 - P_7
$$
由此计算出 $C = \begin{bmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \end{bmatrix}$

算法复杂度 $O(n) = 7O(\frac{n}{2}) + n$ 为 $O(n^{\log \frac{7}{2}})$ 



##### （2）基于软件优化的方法对矩阵乘法进行优化。

​	根据 gemm 有如下两个优化

	1. 将矩阵乘法 $1 \times4$ 展开，代码可以写成

```c++
for (int m = 0; m < M; m++) {
    for (int n = 0; n < N; n += 4) {
        C[m][n + 0] = 0;
        C[m][n + 1] = 0;
        C[m][n + 2] = 0;
        C[m][n + 3] = 0;
        for (int k = 0; k < K; k++) {
            C[m][n + 0] += A[m][k] * B[k][n + 0];
            C[m][n + 1] += A[m][k] * B[k][n + 1];
            C[m][n + 2] += A[m][k] * B[k][n + 2];
            C[m][n + 3] += A[m][k] * B[k][n + 3];
        }
    }
}
```

​	将 `A[m][k]` 放入寄存器中，可以有效减少访存量。

​	同理，将矩阵乘法 $4 \times 4$ 展开，伪代码可以写成，将 `A[m + 0..3][k]` 和 `B[k][n + 0..3]` 放入寄存器，可以更加有效减少访存次数。

```c++
for (int m = 0; m < M; m += 4) {
    for (int n = 0; n < N; n += 4) {
        C[m + 0][n + 0..3] = 0;
        C[m + 1][n + 0..3] = 0;
        C[m + 2][n + 0..3] = 0;
        C[m + 3][n + 0..3] = 0;
        for (int k = 0; k < K; k++) {
            C[m + 0][n + 0..3] += A[m + 0][k] * B[k][n + 0..3];
            C[m + 1][n + 0..3] += A[m + 1][k] * B[k][n + 0..3];
            C[m + 2][n + 0..3] += A[m + 2][k] * B[k][n + 0..3];
            C[m + 3][n + 0..3] += A[m + 3][k] * B[k][n + 0..3];
        }
    }
}
```

​	对矩阵 $A$ 和 $B$ 展开的时候可以复用矩阵 A 和矩阵 B 的数据达到加速效果，尝试将 $k$ 也展开，

```c++
for (int m = 0; m < M; m += 4) {
    for (int n = 0; n < N; n += 4) {
        C[m + 0..3][n + 0..3] = 0;
        C[m + 0..3][n + 0..3] = 0;
        C[m + 0..3][n + 0..3] = 0;
        C[m + 0..3][n + 0..3] = 0;
        for (int k = 0; k < K; k += 4) {
            C[m + 0..3][n + 0..3] += A[m + 0..3][k + 0] * B[k + 0][n + 0..3];
            C[m + 0..3][n + 0..3] += A[m + 0..3][k + 1] * B[k + 1][n + 0..3];
            C[m + 0..3][n + 0..3] += A[m + 0..3][k + 2] * B[k + 2][n + 0..3];
            C[m + 0..3][n + 0..3] += A[m + 0..3][k + 3] * B[k + 3][n + 0..3];
        }
    }
}
```

这里对矩阵 $C$ 会有大量复用，将 $C$ 写入寄存器中可以减少对矩阵 $C$ 的访存操作，提升计算效率。

同时循环展开也对流水线调度友好，能提升流水线效率。

然后还可以利用矩阵相乘向量化进一步提高计算效率，这里需要大量使用指令集，就不在本实验中实现了。

最后还可以针对 cache 进行优化，通过处理内存布局，减少 cache miss 的情况，提高效率。伪代码如下：

```c++
for (int mo = 0; mo < M; mo += 8) {
    for (int no = 0; no < N; no += 8) {
        for (int mi = 0; mi < 2;mi ++) {
            for (int ni = 0; ni < 2; ni++) {
                int m = mo + mi * 4;
                int n = no + ni * 4;
                C[m + 0..3][n + 0..3] = 0;
                C[m + 0..3][n + 0..3] = 0;
                C[m + 0..3][n + 0..3] = 0;
                C[m + 0..3][n + 0..3] = 0;
                for (int k = 0; k < K; k += 4) {
                    C[m + 0..3][n + 0..3] += A[m + 0..3][k + 0] * B[k + 0][n + 0..3];
                    C[m + 0..3][n + 0..3] += A[m + 0..3][k + 1] * B[k + 1][n + 0..3];
                    C[m + 0..3][n + 0..3] += A[m + 0..3][k + 2] * B[k + 2][n + 0..3];
                    C[m + 0..3][n + 0..3] += A[m + 0..3][k + 3] * B[k + 3][n + 0..3];
                }
            }
        }
    }
}
```

最后实现的代码也和伪代码表现出来的一样，代码大量循环展开，冗长且无意义，就不再赘述。



##### 实际上我个人认为有另一种优化形式，这种优化方式主要是针对对 cache 的优化。

对于朴素的矩阵乘法，我们的代码是这样

```c++
for(int i = 0; i < n; ++i)
    for(int j = 0; j < n; ++j)
        for(int k = 0; k < n; ++k)
            C[i][j] += A[i][k] * B[k][j];
```

​	这个做法有一个很大的问题就是当 $k$ 增加时，`B[k][j]` 不连续，不符合局部性原理，会导致大量的 cache miss，大量 cache miss 会导致性能损失严重，只需要简单调换循环执行顺序就可以避免这个问题。

```c++
for(int i = 0; i < n; ++i)
    for(int k = 0; k < n; ++k) {
        float x = A[i][k];
    	for(int j = 0; j < n; ++j)
        	C[i][j] += x * B[k][j];
    }
```

​	这样就能保证数据读取的局部性，大大减少 cache miss， 提高性能。

​	循环展开同样可以优化，但是考虑到循环展开的同时不能让算法局部性变差，否则就违背了优化的初衷，因此主要是对第三层进行展开。最终代码表示为。

```c++
for(int i = 0; i < n; ++i)
    for(int k = 0; k < n; k+=2) {
        float x1 = A[i + 0][k + 0];
        float x2 = A[i + 0][k + 1];
    	for(int j = 0; j < n; j+=4) {
        	C[i + 0][j + 0] += x * B[k + 0][j + 0];
            C[i + 0][j + 1] += x * B[k + 0][j + 1];
            C[i + 0][j + 2] += x * B[k + 0][j + 2];
            C[i + 0][j + 3] += x * B[k + 0][j + 3];
            
            C[i + 0][j + 0] += x * B[k + 1][j + 0];
            C[i + 0][j + 1] += x * B[k + 1][j + 1];
            C[i + 0][j + 2] += x * B[k + 1][j + 2];
            C[i + 0][j + 3] += x * B[k + 1][j + 3];
        }
    }
```

​	该代码唯一的缺点是无法在计算过程中初始化矩阵 $C$ ，需要提前对矩阵 $C$ 进行初始化，初始化矩阵复杂度为 $O(N^2)$ ，矩阵运算复杂度为 $O(N^3)$ ，初始化矩阵可以使用 memset 函数进行快速初始化，不会影响最坏复杂度。实际计算中，计算同样阶矩阵的情况下，该优化也比 gemm 优化效率更高，后文会给出效率对比。



##### （3）intel MKL 函数库

​	调用 intel mkl 函数库中的矩阵乘法函数，代码如下

```c++
#include <mkl/mkl.h>
class MKL_solution {
    public:
    template<int N, int M, int K>
    static MKL_Matrix<N, K> solver(const MKL_Matrix<N, M> &lhs, const MKL_Matrix<M, K> &rhs) {
        MKL_Matrix<N, K> res;
        cblas_sgemm(CblasRowMajor, CblasNoTrans , CblasNoTrans, M, N, K, 1, lhs.data, K, rhs.data, N, 0, res.data, N);
        return res;
    }
};
```

​	

#### 大规模矩阵计算优化

​	如何让程序支持大规模矩阵乘法？

​	对于大规模矩阵乘法，我做一个假设：需要进行 ”大规模矩阵“ 运算的矩阵是稀疏矩阵，针对稀疏矩阵，我们利用其“稀疏”的性质，只保存非零元素的位置而不是整个矩阵。我使用一个结构体分别保存该元素的行，列，值。

```c++
struct eleInMatrix {
    int row,col;
    float val;
};
```



​	针对矩阵乘法公式 $C_{ij} = \sum\limits_{k}A_{ik}B_{kj}$ ，如果我们确定 $A_{ij}$ ，那么 $\forall k \,\,B_{jk}$ 会对 $C_{ik}$ 做出贡献，也就是说我们可以写出如下伪代码 

```c++
for A[i][j] in nodes:
	for B[j][k] in nodes:
		if i == j:
			C[i][k] += A[i][j] * b[j][k]
```

​	这样的算法效率是 $O(n^2)$ ，其中 $n$ 是稀疏矩阵中非 0 单位数。

​	但是，算法仍有改进的空间。考虑到如果确定了 $A_{ij}$ ，那么只有 $B$ 的第 $j$ 行才会做贡献，因此可以使用数据结构将第 $j$ 行的元素存放起来，伪代码如下

```c++
for A[i][j] in nodes:
	for B[j][k] in 第j行元素:
		C[i][k] += A[i][j] * B[j][k];
```

​	实际代码使用哈希表实现，实际代码实现如下。

```c++
struct eleInMatrix {
    int row,col;
    float val;
    eleInMatrix(int row, int col, float val): row(row), col(col), val(val) {}
};
class sparseMatrix {
    std::list<eleInMatrix> data;
};
static sparseMatrix solve(const sparseMatrix &lhs, const sparseMatrix &rhs) {
        std::unordered_map<int, std::unordered_map<int, float> > rmat, tot;
        for(auto &x : rhs.data)
            rmat[x.row][x.col] = x.val;
        for(auto &x: lhs.data) {
            int r = x.row, c = x.col;
            for(auto &p : rmat[c]) {
                tot[r][p.first] += x.val * p.second; 
            }
        }
        std::list<eleInMatrix> res;
        for(auto &x : tot) {
            int r = x.first;
            for(auto &data : x.second) {
                int c = data.first;
                float siz = data.second;
                res.push_back(eleInMatrix(r, c, siz));
            }
        }
        return sparseMatrix(res);
    }
```



#### 实验结果

##### 实验代码正确性

随机生成大小为 size 的数据，输出四种矩阵运算的答案，以及 intel-mkl 得到的数据，因为 gemm 展开了 8 位，不妨让 $size = 8$ ，最后得到的结果均相同，由此认为实验代码正确。下文为 check 代码与，输出结果。

```c++
template<int size>
void check() {
    #define _CHECK
    vector<vector<float>> lData(size,vector<float>(size)), rData(lData);
    for(int i=0;i<size;++i) {
        for(int j=0;j<size;++j) {
            lData[i][j] = rnd()%11;
            rData[i][j] = rnd()%11;
        }
    }
    Matrix<size,size> aLeft(lData), aRight(rData);
    MKL_Matrix<size,size> bLeft(lData), bRight(rData);

    cout<<"start normal algorithm"<<endl;
    cout<<solution::Regular_v1(aLeft,aRight)<<endl;

    cout<<"start Strassen algorithm"<<endl;
    cout<<solution::Strassen(aLeft,aRight)<<endl;

    cout<<"start gemm algorithm"<<endl;
    cout<<solution::Regular_v2(aLeft,aRight)<<endl;

    cout<<"start My algorithm based on cache optimization"<<endl;
    cout<<solution::Regular_v3(aLeft,aRight)<<endl;

    cout<<"start intel mkl algorithm"<<endl;
    cout<<MKL_solution::solver(bLeft,bRight)<<endl;
    #undef _CHECK
}
```



```
start normal algorithm
120 200 194 107 199 298 253 175
102 192 138 126 151 210 224 156
131 306 262 214 238 315 363 292
113 248 194 135 194 272 236 179
62 222 162 118 130 187 168 158
136 246 246 101 236 305 241 183
101 246 152 173 184 267 260 173
99 184 136 127 166 203 207 140

start Strassen algorithm
120 200 194 107 199 298 253 175
102 192 138 126 151 210 224 156
131 306 262 214 238 315 363 292
113 248 194 135 194 272 236 179
62 222 162 118 130 187 168 158
136 246 246 101 236 305 241 183
101 246 152 173 184 267 260 173
99 184 136 127 166 203 207 140

start gemm algorithm
120 200 194 107 199 298 253 175
102 192 138 126 151 210 224 156
131 306 262 214 238 315 363 292
113 248 194 135 194 272 236 179
62 222 162 118 130 187 168 158
136 246 246 101 236 305 241 183
101 246 152 173 184 267 260 173
99 184 136 127 166 203 207 140

start My algorithm based on cache optimize
120 200 194 107 199 298 253 175
102 192 138 126 151 210 224 156
131 306 262 214 238 315 363 292
113 248 194 135 194 272 236 179
62 222 162 118 130 187 168 158
136 246 246 101 236 305 241 183
101 246 152 173 184 267 260 173
99 184 136 127 166 203 207 140

start intel mkl algorithm
120 200 194 107 199 298 253 175
102 192 138 126 151 210 224 156
131 306 262 214 238 315 363 292
113 248 194 135 194 272 236 179
62 222 162 118 130 187 168 158
136 246 246 101 236 305 241 183
101 246 152 173 184 267 260 173
99 184 136 127 166 203 207 140
```

对于稀疏矩阵的计算，我手动构造矩阵，然后 check 是否与结果一致。check 代码如下

```c++
void checkSparse() {
    eleInMatrix a(1,1,2);
    eleInMatrix b(2,2,3);
    eleInMatrix c(1,2,4);
    eleInMatrix d(4,1,6);
    eleInMatrix e(3,3,2);
    sparseMatrix xx({a,b,c,d,e});
    Matrix<4,4> xxx({{2,4,0,0},{0,3,0,0},{0,0,2,0},{6,0,0,0}});
    cout<<solution::Regular_v3(xxx,xxx)<<endl;
    cout<<sparse_solution::solve(xx,xx)<<endl;
}
```

​	上述代码构造了一个矩阵 $A = \begin{bmatrix}2 & 4 & 0 & 0  \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 6 & 0 & 0 & 0  \end{bmatrix}$ ，然后进行计算，计算结果如下。

```
4 20 0 0
0 9 0 0
0 0 4 0
12 24 0 0

3 3 4
4 1 12
4 2 24
2 2 9
1 1 4
1 2 20
```

​	分别表示矩阵运算结果，和稀疏矩阵计算结果（分别表示行，列，值）。

​	可以发现两个结果是一一对应的。因此验证了稀疏矩阵计算的正确性。



##### 性能比较

通过计算得到结果（均为计算 5 次取平均值）**其中表格数据单位为（1/秒）**

| 矩阵算法\矩阵规模  | 128       | 256       | 512       | 1024      | 2048     |
| ------------------ | --------- | --------- | --------- | --------- | -------- |
| mkl                | 0.0079598 | 0.0088444 | 0.0043526 | 0.0216834 | 0.140727 |
| cache optizization | 0.0203302 | 0.1670286 | 1.3519620 | 11.146620 | 84.9734  |
| Strassen           | 0.0213274 | 0.1617622 | 1.1900700 | 8.3672580 | 55.49674 |
| gemm               | 0.0310690 | 0.2703286 | 2.3343840 | 18.717360 | 150.9368 |
| normal             | 0.0293130 | 0.2508194 | 2.5211560 | 25.103040 | 240.4484 |

下图为当 $size=512$ 时候的计算结果

<img src="C:\Users\zengbi\AppData\Roaming\Typora\typora-user-images\image-20210909200245153.png" alt="image-20210909200245153" style="zoom:67%;" />

​	可以看到 intel-mkl 速度最快，其次是 Strassen 算法，然后是基于 cache 优化的算法，然后是 gemm 优化，最朴素的矩阵乘法算法最慢。

​	由于 mkl 可能使用了大量的指令集优化，而 c++ 默认编译优化不会优化到指令集、寄存器级别，因此开启 `-O3` 优化再次进行实验，**其中表格数据单位为（1/秒）**

| 矩阵算法\矩阵规模  | 128       | 256       | 512       | 1024      | 2048      | 4096     |
| ------------------ | --------- | --------- | --------- | --------- | --------- | -------- |
| mkl                | 0.0017836 | 0.0020950 | 0.004212  | 0.0200986 | 0.1448932 | 1.123600 |
| cache optizization | 0.0004632 | 0.0026618 | 0.0244296 | 0.2118498 | 2.621676  | 28.07018 |
| Strassen           | 0.0007264 | 0.0042934 | 0.0323798 | 0.2377154 | 1.806960  | 12.94454 |
| gemm               | 0.0014506 | 0.0099036 | 0.093977  | 0.8726134 | 9.026778  | 84.20452 |
| normal             | 0.0027408 | 0.0209314 | 0.1927894 | 1.719784  | 66.30966  | 时间过长 |

​	可以看到运行速度明显提高，但是仍然遵守效率 mkl > Strassen > cache > gemm > normal 的规律。

##### 分析原因

​	mkl 作为成熟的科学计算库，应当是使用了大量的指令集优化，可以发现 mkl 库在开启 `-O3` 优化后几乎没有性能提升，可以认为 mkl 库几乎优化了 -O3 能优化的所有操作。速度上与 mkl 库最接近的是 Strassen 算法， 在 $4096\times4096$ 的矩阵乘法中，仅相差 $10$ 倍的差距。猜测 mkl 库使用了 Coppersmith–Winograd 或者理论复杂度更低的其他算法。使用向量化可以一个指令同时计算 2 次加法和 2 次乘法，理论复杂度和常数同时减少，我优化后的算法还是可能达到 mkl 的水平的。

​	实验表明基于 cache 优化的算法效率高于 gemm ，随着矩阵大小的增大，基于 cache 优化的算法会有更少的 cache miss，可能 gemm 减少了访存次数，但是相比于访存次数增多，cache miss 似乎代价更高。

​	Strassen 算法由于递归算法的特殊性，递归深度过高会对算法效率产生严重影响，因此小于一个阙值会调用基于 cache 优化的算法（我设定的是 128），数据量较大时，似乎也取得了不错的效率。



#### 实验感想

​	实验中遇到的最大的问题应该就是判断非法矩阵运算问题。这次实验我在学习矩阵乘法的同时，也希望可以给我的 c++ 练练手。为保证运算“安全性”，我选择使用模板类，这样编译期间就检测出非法矩阵运算，保证运行期间的安全性。但是更多的问题也随之而来，如果矩阵大小不固定，计算过程会出现增大/减少矩阵大小的情况，模板类似乎难以胜任。

​	intel-mkl 库似乎可以解决矩阵大小变化的问题，但是使用体验极差，参数表长达 10 个，理解每个参数的含义都要花费不少学习时间，而且接口函数也没有体现出“矩阵”的概念。

​	想设计一个既安全，又便于使用的矩阵乘法，似乎不那么容易，其中可能更多精心设计，以后又时间的话我再去进一步研究。

​	同时指令集会大大增加矩阵乘法效率，MKL 就是使用了大量的汇编加快效率，可以看到 intel MKL 效率基本上已经提升倒了极限，O3 优化也无法再进一步提升 MKL 的效率，内联汇编在效率提升上起到了至关重要的作用。不过 c++ 可以兼容 c ，所以我认为能在 c++ 上面实现高效率矩阵乘法是完全可能的。总所周知，直到 c++23 标准都没有将矩阵运算库加入 c++ 标准中，我认为类似与我这样的模板写法可能是个不错的选择，总之期待不久后矩阵运算库能加入 c++ 标准。
