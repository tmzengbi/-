## 中山大学计算机院本科生实验报告

#### **（2021 学年秋季学期）**

课程名称：高性能计算程序设计基础                   **批改人：**

| 实验  | **Lab3**                                          | 专业（方向） | **信息与计算科学** |
| ----- | ------------------------------------------------- | ------------ | ------------------ |
| 学号  | **18323004**                                      | 姓名         | **曾比**           |
| Email | **[2818097988@qq.com](mailto:2818097988@qq.com)** | 完成日期     |                    |

### 实验目的

0. **构造MPI版本矩阵乘法加速比和并行效率表**
1. **通过 Pthread 实现通用矩阵乘法**
2. **基于Pthread 的数组求和**
3. **Pthread 求解二次方程组的根**
4. **编写一个多线程程序来**

### 实验过程和核心代码

#### 实验环境

```
Window10 WSL2 下 Linux 子系统 Ubuntu20.04，CPU 为 AMD Ryzen7 5800H，8核16线程，gcc9.3.0
```

​	在实验中，为了方便控制线程数量。将线程数量作为参数进行传递。同时为了方便将字符串转换为数字，使用了 `boost` 库中的 `boost/lexical_cast` 。

#### 构造MPI版本矩阵乘法加速比和并行效率表（优化了上一次实验的代码）

​	相比上一次的 2 核 CPU，这次使用了核数更多 AMD Ryzen7，因此数据和上一次有所不同，重新进行实验得到数据。

​	而且上次实验有许多考虑不足的地方，更是没有实现 `create struct` ，后面才发现，我没有严格遵循老师给出的输入格式，理想输入格式应该为。

```
N M K
matrix N * M // N rows and M cols
matrix M * K // M rows and K cols
```

​	表示输入 $N \times M $ 矩阵 $A$ 和 $M \times K$ 的矩阵 $B$。目的是求出 $A \times B$ 。

​	如果简单使用点对点，或者聚合通信，那么 `N` ，`M` ，`K` 以及 $2$ 个矩阵各需要传输一次，一共需要传输 $5$ 次数据（上次实验就是大大简化了输入过程，结果错误地判断不需要 `create struct` ）。这个时候 `create struct` 就大有可为了。

​	为了方便使用脚本进行自动化操作，以及对一些代码进行优化，上一个实验的代码将进行一些简单的修改，主要修改如下。

- 优化分配算法

> 本实验中，仍然使用块分配。每个进程可以分配到 $\lceil \frac{N}{size} \rceil \times M$ 的矩阵。如果 $size \nmid N$  ，那么补充 $n\times M$ 的 $0$ 矩阵使得 $size \mid N + n$ 。这样就避免了繁琐的特判。对效率影响尽可能小的情况下，大大提高代码可读性。
>
> 同时在点对点通信中，不在让 $0$ 号进程单纯作为通信的角色，也就是说 $0$ 号进程也要加入运算。

##### 点对点

读取完 $N$ , $M$ , $K$ 后，求出 $n =\lceil \frac{N}{size} \rceil $ , $m = M$ , $k = K$ ，并分配内存。然后为每个进程通信。

每个进程都需要求 $n\times m$ 矩阵和 $m\times k$ 矩阵乘积。核心代码如下。

```c++
if(rank == 0) {
    int n = (N + siz - 1) / siz, m = M, k = K;
    int realN = n * siz;
    float *A, *B, *C;
    A = (float*) malloc(sizeof(float) * realN * m);
    B = (float*) malloc(sizeof(float) * m * k);
    C = (float*) malloc(sizeof(float) * realN * K); 
    for(int i = 1; i < siz; ++ i) {
        MPI_Send(&n, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
        MPI_Send(&m, 1, MPI_INT, i, 1, MPI_COMM_WORLD);
        MPI_Send(&k, 1, MPI_INT, i, 2, MPI_COMM_WORLD);
        MPI_Send(A + n * m * i, n * m, MPI_FLOAT, i, 3, MPI_COMM_WORLD);
        MPI_Send(B, m * k, MPI_FLOAT, i, 4, MPI_COMM_WORLD);
    }
    solve(A, B, C, n, m, k);
        for(int i = 1; i < siz; ++ i)
            MPI_Recv(C + i * n * k, n * k, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
} else {
    MPI_Recv(&n, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(&m, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(&k, 1, MPI_INT, 0, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(A, n * m, MPI_FLOAT, 0, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    MPI_Recv(B, m * k, MPI_FLOAT, 0, 4, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    solve(A, B, C, n, m, k);
    MPI_Send(C, n * k, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);
}
```

##### 聚合通信

原理同点对点通信，使用聚合通信函数替换。

```c++
MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);
MPI_Bcast(&m, 1, MPI_INT, 0, MPI_COMM_WORLD);
MPI_Bcast(&k, 1, MPI_INT, 0, MPI_COMM_WORLD);
if(rank != 0)
    B = (float*) malloc(m * k * sizeof(float));
float *x, *res;
x = (float*) malloc(n * m * sizeof(float));
res = (float*) malloc(n * k * sizeof(float));
MPI_Scatter(A, n * m, MPI_FLOAT, x, n * m, MPI_FLOAT, 0, MPI_COMM_WORLD);
MPI_Bcast(B, m * k, MPI_FLOAT, 0, MPI_COMM_WORLD);
solve(x, B, res, n, m, k);
MPI_Gather(res, n * k, MPI_FLOAT, C, n * k, MPI_FLOAT, 0, MPI_COMM_WORLD);
```

##### mpi_create_struct

​	聚合通信有四个 `MPI_Bcast` ，但是只能将 `n` , `m` , `k` 放入新集合中一起传输，再传输矩阵 `B` ，因为除了根进程 （`0` 号进程）在读取完 `n` , `m` , `k` 后就给 `B` 矩阵分配完空间，其他进程都没有给矩阵 `B` 分配空间。因此代码和聚合通信几乎类似，将聚合通信的对于 `n` , `m` , `k` 数据的 `MPI_Bcast` 修改为如下。

```c++
MPI_Aint n_addr, m_addr, k_addr;
MPI_Address(&n, &n_addr);
MPI_Address(&m, &m_addr);
MPI_Address(&k, &k_addr);
const int len[3] = {1, 1, 1};
const MPI_Aint arr[3] = {0, m_addr - n_addr, k_addr - n_addr};
MPI_Datatype types[3] = {MPI_INT, MPI_INT, MPI_INT};
MPI_Datatype newtype;
MPI_Type_create_struct(3, len, arr, types, &newtype);
MPI_Type_commit(&newtype);
MPI_Bcast(&n, 1, newtype, 0, MPI_COMM_WORLD);
MPI_Type_free(&newtype);
```

​	这样实现。每个进程从 $5$ 次通信缩减为 $3$ 次通信，提高效率。

#### 通过 Pthread 实现通用矩阵乘法

​	这次计划使用两种不同的算法实现 `pthread` 并行通用矩阵乘法。

​	在上一个 `mpi` 实现并行实验中，使用了块划分的思路，但是结合网上其他人的思路，发现循环划分实现并行化看起来更加方便，现在依然是平均每个进程分配 $\frac{n}{size} \times size$ 的矩阵（其中 $n$ 表示并行数量，$size$ 表示矩阵的秩） ，使用循环划分，我可以更加快速定位到分配给当前进程的矩阵向量。如下列代码所示（ `id` 表示当前进程号，$A$ $B$ 表示矩阵，我需要计算 $A\times B$ 的结果）

```c++
for(int i = id; i < size; i += n) {
    solve(A[i], B); // A矩阵第i行表示的向量和B矩阵相乘。
}
```

​	但是这个做法也有不足的地方，和块划分最大的差异就是，块划分得到的块都是连续的，而循环划分得到的块都是不连续的，理论上来说，块划分对 `cache` 更加友好，循环划分面临着更多的 `cache miss` 后果。

​	因此我还会实现块划分来观察结果，对比两者效率并分析原因。

​	此外，由于需要计算算法执行时间来估算算法效率，此时 `clock` 函数失效（`clock` 只有在单线程才能保证正确结果）。因此考虑使用 `std::chrono::steady_clock` 来得到精确到微秒的时间。

##### 循环划分

```c++
int N, M, K;
vector<float> A, B, C; 
// C = A * B
// A is N * M .. B is M * K .. C is N * K
void *solve(void *argv) { 
    int id = *(int*) argv;
    #define idA (i * M + k)
    #define idB (k * K + j)
    #define idC (i * K + j)
    for(int i = id; i < N; i += siz)
        for(int k = 0; k < M; ++ k) {
            float x = A[idA];
            for(int j = 0; j < K; ++ j) {
                C[idC] += x * B[idB];
            }
        }
    return NULL;
}
```

##### 块划分

```c++
int N, M, K;
vector<float> A, B, C; // A is N * M .. B is M * K .. C is N * K
void *solve(void *argv) { 
    int id = *(int*) argv;
    #define idA (i * M + k)
    #define idB (k * K + j)
    #define idC (i * K + j)
    // cout<<id<<endl;
    int block = (N + siz - 1) / siz; 
    for(int i = id; i < id + block; ++ i) {
        if(i >= N) break;
        for(int j = 0; j < K; ++ j) {
            C[idC] = 0; // C[i][j] = 0;
            for(int k = 0; k < M; ++ k)
                C[idC] += A[idA] * B[idB]; // C[i][j] += A[i][k] * B[k][j];
        }
    }
    return NULL;
}
```



#### 基于 Pthread 的数组求和

​	基于 `pthread` 数组求和。这里要求我每次只能有一个进程访问全局变量 `global_index`，依次用两种方法实现。

- 每个线程通过共享单元 `global_index` 获取 `a` 数组的下一个未加元素

根据需求，对 `global_index` 的访问需要在临界区内，因此每次调用 `global_index` 变量的时候，都需要加锁。同时使用了 `sum` 数组存储每个线程的答案。核心代码如下。

```c++
void *solve(void *argv) {
    int *sum = (int*) argv;
    while(true) {
        pthread_mutex_lock(&mutex);
        if(global_index == MAXN) {
            pthread_mutex_unlock(&mutex);
            break;
        }
        int id = global_index++;
        pthread_mutex_unlock(&mutex);
        *sum += a[id];
    }
    return NULL;
}
```

- 各进程可以一次最多提取 $10$ 个连续的数。

考虑到求和部分不需要在临界区内，将求和部分代码放在临界区外可以加快 `global_index` 的访问速度。

```c++
void *solve(void *argv) {
    int *sum = (int*) argv;
    while(true) { 
        pthread_mutex_lock(&mutex);
        if(global_index == MAXN) {
            pthread_mutex_unlock(&mutex);
            break;
        }
        int id = global_index;
        global_index += 10;
        pthread_mutex_unlock(&mutex);
        for(int i = 0; i < 10; ++ i)
            *sum += a[id + i];
    }
    return NULL;
}
```



#### Pthread 求解二次方程组的根

​	考虑 $x=\frac{-b \pm \sqrt{b^2-4ac}}{2a}$ ，主要中间过程是求  $\Delta=b^2-4ac$ ，于是使用一个进程专门求 $\Delta$ ，然后在开两个进程分别求 $x_1=\frac{-b + \sqrt{b^2-4ac}}{2a}$ 和 $x_2=\frac{-b - \sqrt{b^2-4ac}}{2a}$ 。

​	三个线程代码分别如下。

##### 计算 $\Delta$

```c++
void *calcDelta(void *argv) {
    delta = b*b-4.0*a*c;
    while(counter < 2);
    if(fabs(delta) < eps) delta = 0;
    pthread_cond_broadcast(&cond_var);
    return NULL;
}
```

##### 计算 $x_1$

```c++
void *getx1(void *argv) {
    res.first = -b / 2.0 / a;
    pthread_mutex_lock(&mutex);
    ++ counter;
    while(pthread_cond_wait(&cond_var, &mutex) != 0);
    pthread_mutex_unlock(&mutex);
    if(delta < -eps) return NULL;
    res.first += sqrt(delta) / 2.0 / a;
    return &res.first;
}
```

##### 计算 $x_2$

```c++
void *getx2(void *argv) {
    res.second = -b / 2.0 / a;
    pthread_mutex_lock(&mutex);
    ++ counter;
    while(pthread_cond_wait(&cond_var, &mutex) != 0);
    pthread_mutex_unlock(&mutex);
    if(delta < -eps) return NULL;
    res.second += -sqrt(delta) / 2.0 / a;
    return &res.second;
}
```

​	其中需要注意的是浮点数加减可能会有精度误差，因此在计算出 $\Delta$ 后，需要清除误差（针对  $\Delta=0$ 但是但是由于计算误差导致 $\Delta < 0$ 的情况，`sqrt` 函数参数小于 $0$ 会报错）



#### 编写一个多线程程序来

​	使用 `Monte-carlo` 方法估计 $y=x^2$ 于 $x$ 轴之间区域的面积。

​	考虑到大多数随机数生成器不是线程安全，因此考虑在每一个进程中使用一个随机数种子，而这个随机数种子可以提前生成好，通过参数传递。考虑到 `rand` 生成出来的随机数范围为 `0-0x7fff`，且随机强度低。因此考虑随机强度更高的 `std::mt19937` 作为随机数生成器。经过代码测试得到，代码如下。这里每个线程的输出结果均相同，可以证明 `mt19937` 并没有使用全局变量或者静态变量来维护随机数种子，这样就可以保证随机数在每个线程中都是独立的，可以起到线程安全的作用。

```c++
void solve() {
    mt19937 rnd(1);
    printf("%lu\n",rnd());
}
int main() {
    thread xx[5];
    for(int i=0;i<5;++i)
        xx[i]=thread(solve);
    for(int i=0;i<5;++i)
        xx[i].join();
}
```

​	在完成随机数生成器的初始化后。接下来安排每个进程的工作，目前认为可以每个进程都随机生成100000 个点对 $(x,y)$ ，然后测试 $(x,y)$ 是否在 $y=x^2$ 下方。

- 主进程

```c++
siz = boost::lexical_cast<int> (argv[1]);
mt19937 rnd(time(NULL));
pthread_mutex_init(&mutex, NULL);
pthread_t thread[siz];
unsigned int seed[siz];
for(int i = 0; i < siz; ++ i) {
    seed[i] = rnd();
    pthread_create(thread + i, NULL, fun, seed + i);
}
```

- 计算进程

其中生成 $[0,1]$ 随机数的方式是随机生成小于等于 `MAXNUM` 的随机数 $x$，然后 $\frac{x}{MAXNUM}$ 一定 $\in [0,MAXNUM]$ 。这里令 $MAXNUM=10000$

```c++
void* fun(void *argv) {
    unsigned int seed = *(unsigned int*) argv;
    mt19937 rnd(seed);
    int sum = 0;
    for(int i = 0; i < MAXN; ++ i) {
        int x = rnd() % (MAXNUM + 1); // num is [0, MAX]
        int y = rnd() % (MAXNUM + 1);
        // (y/M) <= (x/M)^2 means My <= x^2
        // printf("%d %d %d %d\n", x, y, MAXNUM * y, x * x);
        if(MAXNUM * y <= x * x) ++ sum;
    }
    tot += sum;
    printf("thread get answer %d/%d=%.10lf\n", sum, MAXN, 1.0 * sum / MAXN);
    return NULL;
}
```



### 实验结果

​	实际情况表示，同样条件下的实验结果极差较大，为了尽量减小误差，每次实验都会进行 $5$ 次，实验结果取 $5$ 次的平均值。

#### 构造MPI版本矩阵乘法加速比和并行效率表

##### 点对点执行时间（单位：秒）

| 进程数量\矩阵大小 | 256      | 512      | 1024     | 2048      |
| ----------------- | -------- | -------- | -------- | --------- |
| 1                 | 0.044335 | 0.359997 | 2.905486 | 23.360249 |
| 2                 | 0.025425 | 0.183309 | 1.536766 | 12.278372 |
| 3                 | 0.015621 | 0.137720 | 1.053854 | 8.473735  |
| 4                 | 0.012229 | 0.104307 | 0.880181 | 6.598249  |
| 5                 | 0.012159 | 0.090784 | 0.701801 | 5.488650  |
| 6                 | 0.010623 | 0.077272 | 0.615294 | 4.804486  |
| 7                 | 0.009659 | 0.075148 | 0.555541 | 4.276887  |
| 8                 | 0.009805 | 0.069877 | 0.511547 | 3.854552  |
| 9                 | 0.008972 | 0.062264 | 0.477519 | 3.593622  |
| 10                | 0.008725 | 0.059129 | 0.447903 | 3.399094  |
| 16                | 0.013162 | 0.051702 | 0.415079 | 2.502370  |

##### 点对点加速比

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 1.74376 | 1.96388 | 1.89065 | 1.90255 |
| 3                 | 2.83817 | 2.61398 | 2.75701 | 2.75678 |
| 4                 | 3.62540 | 3.45132 | 3.30101 | 3.54037 |
| 5                 | 3.64627 | 3.96542 | 4.14004 | 4.25610 |
| 6                 | 4.17349 | 4.65883 | 4.72211 | 4.86217 |
| 7                 | 4.59002 | 4.79051 | 5.23001 | 5.46197 |
| 8                 | 4.52167 | 5.15187 | 5.67980 | 6.06043 |
| 9                 | 4.94148 | 5.78178 | 6.08455 | 6.50047 |
| 10                | 5.08138 | 6.08833 | 6.48686 | 6.87249 |
| 16                | 3.36841 | 6.96292 | 6.99984 | 9.33525 |

##### 点对点效率

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 0.87188 | 0.98194 | 0.94532 | 0.95127 |
| 3                 | 0.94606 | 0.87133 | 0.91900 | 0.91893 |
| 4                 | 0.90635 | 0.86283 | 0.82525 | 0.88509 |
| 5                 | 0.72925 | 0.79308 | 0.82801 | 0.85122 |
| 6                 | 0.69558 | 0.77647 | 0.78702 | 0.81036 |
| 7                 | 0.65572 | 0.68436 | 0.74714 | 0.78028 |
| 8                 | 0.56521 | 0.64398 | 0.70998 | 0.75755 |
| 9                 | 0.54905 | 0.64242 | 0.67606 | 0.72227 |
| 10                | 0.50814 | 0.60883 | 0.64869 | 0.68725 |
| 16                | 0.30622 | 0.63299 | 0.63635 | 0.84866 |

##### 聚合通信执行时间（单位：秒）

| 进程数量\矩阵大小 | 256      | 512      | 1024     | 2048      |
| ----------------- | -------- | -------- | -------- | --------- |
| 1                 | 0.042618 | 0.334292 | 2.667665 | 21.322791 |
| 2                 | 0.021858 | 0.171793 | 1.412318 | 11.193417 |
| 3                 | 0.014965 | 0.117672 | 1.032557 | 8.212209  |
| 4                 | 0.011306 | 0.105779 | 0.904325 | 6.571884  |
| 5                 | 0.011899 | 0.085759 | 0.717092 | 5.364463  |
| 6                 | 0.008279 | 0.079617 | 0.621629 | 4.612668  |
| 7                 | 0.007922 | 0.069590 | 0.569402 | 4.122855  |
| 8                 | 0.009617 | 0.068930 | 0.499088 | 3.769577  |
| 9                 | 0.008804 | 0.063380 | 0.465806 | 3.473032  |
| 10                | 0.007944 | 0.059574 | 0.442177 | 3.289280  |
| 16                | 0.006982 | 0.042034 | 0.323526 | 2.572880  |

##### 聚合通信加速比

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 1.94977 | 1.94590 | 1.88886 | 1.90494 |
| 3                 | 2.84784 | 2.84088 | 2.58355 | 2.59647 |
| 4                 | 3.76950 | 3.16029 | 2.94990 | 3.24455 |
| 5                 | 3.58165 | 3.89804 | 3.72012 | 3.97482 |
| 6                 | 5.14772 | 4.19875 | 4.29141 | 4.62266 |
| 7                 | 5.37970 | 4.80374 | 4.68503 | 5.17185 |
| 8                 | 4.43153 | 4.84973 | 5.34508 | 5.65655 |
| 9                 | 4.84075 | 5.27441 | 5.72699 | 6.13953 |
| 10                | 5.36480 | 5.61137 | 6.03303 | 6.48251 |
| 16                | 6.10398 | 7.95290 | 8.24560 | 8.28752 |

##### 聚合通信效率

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 0.97489 | 0.97295 | 0.94443 | 0.95247 |
| 3                 | 0.94928 | 0.94696 | 0.86118 | 0.86549 |
| 4                 | 0.94237 | 0.79007 | 0.73747 | 0.81114 |
| 5                 | 0.71633 | 0.77961 | 0.74402 | 0.79496 |
| 6                 | 0.85795 | 0.69979 | 0.71523 | 0.77044 |
| 7                 | 0.76853 | 0.68625 | 0.66929 | 0.73884 |
| 8                 | 0.55394 | 0.60622 | 0.66814 | 0.70707 |
| 9                 | 0.53786 | 0.58605 | 0.63633 | 0.68217 |
| 10                | 0.53648 | 0.56114 | 0.60330 | 0.64825 |
| 16                | 0.55491 | 0.72299 | 0.74960 | 0.75341 |

##### mpi_create_struct 执行时间（单位：秒）

| 进程数量\矩阵大小 | 256      | 512      | 1024     | 2048      |
| ----------------- | -------- | -------- | -------- | --------- |
| 1                 | 0.044079 | 0.354502 | 2.813679 | 22.586582 |
| 2                 | 0.022512 | 0.179129 | 1.519671 | 11.940574 |
| 3                 | 0.015589 | 0.123119 | 0.998240 | 8.428282  |
| 4                 | 0.012030 | 0.094194 | 0.849921 | 6.563077  |
| 5                 | 0.009989 | 0.089126 | 0.680544 | 5.463023  |
| 6                 | 0.008376 | 0.074180 | 0.670810 | 4.823755  |
| 7                 | 0.008833 | 0.071631 | 0.566434 | 4.271543  |
| 8                 | 0.009612 | 0.071361 | 0.530073 | 3.918461  |
| 9                 | 0.008608 | 0.064966 | 0.474125 | 3.612416  |
| 10                | 0.008075 | 0.061799 | 0.454685 | 3.316524  |
| 16                | 0.010506 | 0.041109 | 0.324410 | 2.526333  |

##### mpi_create_struct 加速比

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 1.95802 | 1.97903 | 1.85151 | 1.89158 |
| 3                 | 2.82757 | 2.87934 | 2.81864 | 2.67986 |
| 4                 | 3.66409 | 3.76353 | 3.31052 | 3.44146 |
| 5                 | 4.41275 | 3.97754 | 4.13446 | 4.13445 |
| 6                 | 5.26254 | 4.77894 | 4.19445 | 4.68237 |
| 7                 | 4.99026 | 4.94900 | 4.96736 | 5.28769 |
| 8                 | 4.58583 | 4.96773 | 5.30810 | 5.76415 |
| 9                 | 5.12070 | 5.45673 | 5.93447 | 6.25249 |
| 10                | 5.45870 | 5.73637 | 6.18819 | 6.81032 |
| 16                | 4.19560 | 8.62346 | 8.67322 | 8.94046 |

##### mpi_create_struct 效率

| 进程数量\矩阵大小 | 256     | 512     | 1024    | 2048    |
| ----------------- | ------- | ------- | ------- | ------- |
| 1                 | 1.00000 | 1.00000 | 1.00000 | 1.00000 |
| 2                 | 0.97901 | 0.98952 | 0.92575 | 0.94579 |
| 3                 | 0.94252 | 0.95978 | 0.93955 | 0.89329 |
| 4                 | 0.91602 | 0.94088 | 0.82763 | 0.86037 |
| 5                 | 0.88255 | 0.79551 | 0.82689 | 0.82689 |
| 6                 | 0.87709 | 0.79649 | 0.69908 | 0.78039 |
| 7                 | 0.71289 | 0.70700 | 0.70962 | 0.75538 |
| 8                 | 0.57323 | 0.62097 | 0.66351 | 0.72052 |
| 9                 | 0.56897 | 0.60630 | 0.65939 | 0.69472 |
| 10                | 0.54587 | 0.57364 | 0.61882 | 0.68103 |
| 16                | 0.38142 | 0.78395 | 0.78847 | 0.81277 |

​	显然上述所有方法都不符合强扩展性。问题规模不变的前提下，增加进程数量会导致并行效率变低。但是上述所有方法都可能具备弱扩展性的特点。原因是维持进程数量不变的前提下，增加问题规模是会增加并行效率的。可能存在一个 $k$ 使得算法进程数量乘上 $1$ 后，问题规模需要乘以 $k$ 才会维持并行效率。

​	同时发现一个有趣的问题，就是当问题规模很大（$n=2048$），并行程度也很大的时候（$p=16$），并行效率不降反升。这个问题暂时想不到解释理由。



#### 通过 Pthread 实现通用矩阵乘法

- 分块

随机输入一个矩阵，不妨令  $A = \begin{bmatrix} 1 & 4  & 7 \\ 5 & 2 & 1\end{bmatrix}$   $B =\begin{bmatrix} 2 & 3  & 2 \\ 1 & 2 & 3 \\ 5 & 2 & 3  \end{bmatrix}$  

得到结果如下，与实际结果相同，认为算法正确。

```
41.000000 25.000000 35.000000
17.000000 21.000000 19.000000
```



| 线程数量\矩阵大小 | 256         | 512        | 1024      | 2048     |
| ----------------- | ----------- | ---------- | --------- | -------- |
| 1                 | 0.04476604  | 0.372391   | 2.86862   | 22.96792 |
| 2                 | 0.0275563   | 0.2223912  | 1.693874  | 12.65712 |
| 3                 | 0.0191654   | 0.1536526  | 1.168818  | 8.613788 |
| 4                 | 0.01382032  | 0.11178742 | 0.8967038 | 6.984002 |
| 5                 | 0.01196886  | 0.1008604  | 0.7495508 | 5.724968 |
| 6                 | 0.009849168 | 0.07426408 | 0.6747182 | 5.006222 |
| 7                 | 0.00951533  | 0.07502664 | 0.592227  | 4.525274 |
| 8                 | 0.01067044  | 0.07080278 | 0.5607732 | 4.21986  |
| 9                 | 0.010146972 | 0.07082334 | 0.5200572 | 3.820922 |
| 10                | 0.00937135  | 0.06980298 | 0.4806536 | 3.517730 |

- 循环划分

同理，使用  $A = \begin{bmatrix} 1 & 4  & 7 \\ 5 & 2 & 1\end{bmatrix}$   $B =\begin{bmatrix} 2 & 3  & 2 \\ 1 & 2 & 3 \\ 5 & 2 & 3  \end{bmatrix}$  

得到如下结果，与实际结果相同，认为算法正确。

```
41.000000 25.000000 35.000000
17.000000 21.000000 19.000000
```



| 线程数量\矩阵大小 | 256         | 512        | 1024      | 2048     |
| ----------------- | ----------- | ---------- | --------- | -------- |
| 1                 | 0.04566106  | 0.3579392  | 2.870776  | 23.00476 |
| 2                 | 0.02417514  | 0.186833   | 1.506732  | 12.16506 |
| 3                 | 0.01644326  | 0.1390734  | 1.041206  | 8.29659  |
| 4                 | 0.01260722  | 0.0956089  | 0.8145086 | 6.650668 |
| 5                 | 0.01021892  | 0.07988168 | 0.7296134 | 5.545048 |
| 6                 | 0.00872068  | 0.07263354 | 0.6400272 | 4.946306 |
| 7                 | 0.00896792  | 0.06624772 | 0.57323   | 4.41292  |
| 8                 | 0.00976969  | 0.0735214  | 0.5076902 | 4.00445  |
| 9                 | 0.008766738 | 0.06411926 | 0.4686134 | 3.678906 |
| 10                | 0.0080223   | 0.05966638 | 0.4417758 | 3.399408 |

​	对比两种做法，虽然想象中可能块划分会有更快的效率，但是实上两个算法表现情况不相上下，效率基本相同。

​	令人出乎意料的是，多线程的表现与多进程在效率方面的表现不相上下，没有体现出明显的效率优势。

#### 基于 Pthread 的数组求和 

首先验证完成代码的正确性。不妨令所有数组元素为 $1$， 启动 $10$ 个线程，数组大小为 $100000$。得到结果如下。

```
thread0 get sum 16212
thread1 get sum 7354
thread2 get sum 15288
thread3 get sum 7852
thread4 get sum 15157
thread5 get sum 6332
thread6 get sum 10047
thread7 get sum 6980
thread8 get sum 6970
thread9 get sum 7808
```

将所有进程的 `sum` 进行求和可以发现结果为 $100000$ ，可以表示代码正确。

基于 `Pthread` 数组求和使用了两种方式，一次只访问一个下标，或者一次访问 $10$ 个下标。在这个基础上，我决定对其效率进行比较。

现在我统一使用 $10$ 个线程，每个数组大小为 $10^8$ 然后每个数组元素为 $1$。 

| 单次求和数量\求和数组大小 | $10^7$    | $10^8$   |
| ------------------------- | --------- | -------- |
| 1                         | 0.194849  | 1.89752  |
| 10                        | 0.0757671 | 0.527737 |

可以看到每次提取 $10$ 次元素求和效率更高。这个模型类似于流水线：每次单独拿出一个数字是

#### Pthread 求解二次方程组的根

实现程序后，随意输出 $ax^2+bx+c=0$ 的 $a$ ， $b$ ， $c$ ，比如 $a=2$ ， $b=4$ ， $c=1$ ，输出如下。

```
-0.292893 -1.70711
```

表示得到  $x_1=-0.292893$  $x_2=-1.70711$ 的结果。



#### 编写一个多线程程序来

我们可以通过数学方式计算得到 $\int_0^1 x^2=\frac{1}{3}$

在实验中，使用 $10$ 个进程，每个进程随机生成 $1000000$ 次随机点。得到结果如下。

```
thread get answer 333159/1000000=0.3331590000
thread get answer 334616/1000000=0.3346160000
thread get answer 332907/1000000=0.3329070000
thread get answer 334261/1000000=0.3342610000
thread get answer 333671/1000000=0.3336710000
thread get answer 332489/1000000=0.3324890000
thread get answer 332344/1000000=0.3323440000
thread get answer 333757/1000000=0.3337570000
thread get answer 333590/1000000=0.3335900000
thread get answer 333194/1000000=0.3331940000
total get answer 3333988/10000000=0.3333988000
```

可以看到结果 $0.3333988000$ 与 $\frac{1}{3}$ 非常接近。

### 实验感想

​	首先，本实验的多线程程序为了提高扩展性，使用了传参的方式读入并行程度，（也就是线程数量），为此我使用了 `boost/lexical_cast.hpp` 来实现字符串到整型的转换。也因为而使用了 `cpp` ，于是我顺其自然的使用了 `std::vector` 来作为矩阵存储的容器。但是实现完代码后，发现在 $2048 \times 2048$ 规模的矩阵乘法运算前提下，使用 `std::vector` 的算法使用了整整 $31$ 秒，而转换为 `float` 指针后只需要 $22$ 秒，足足提高了进 $\frac{1}{3}$ 左右的效率。这一点可以很明显的体现出 `CPP` 与 `C` 的运行效率差距。因此高性能程序还是尽量使用 `C` 语言风格实现。

 	其次，整个实验最大的感想就是上个实验报告我曾经说过多线程可能会比多进程快，但是实际表现似乎并非如此。在同等问题规模和同样的并行程度的前提下，多进程和多线程的时间运行时间似乎相差无几，想象中的 `多进程会因为通信以及数据传输导致效率低下` 的情况并没有在本实验中发生。这说明 `mpi` 实现多进程并行，至少在矩阵乘法上面是可靠的。

​	
