## 中山大学计算机院本科生实验报告

#### **（2021 学年秋季学期）**

课程名称：高性能计算程序设计基础                   **批改人：**

| 实验  | **Lab2**                                          | 专业（方向） | **信息与计算科学** |
| ----- | ------------------------------------------------- | ------------ | ------------------ |
| 学号  | **18323004**                                      | 姓名         | **曾比**           |
| Email | **[2818097988@qq.com](mailto:2818097988@qq.com)** | 完成日期     |                    |

### 实验目的

1. 通过MPI实现通用矩阵乘法

2. 基于MPI的通用矩阵乘法优化

3. 改造Lab1成矩阵乘法库函数

### 实验过程和核心代码

##### 实验环境

> 实验环境为：Ubuntu20.04，gcc9.3.0，阿里云 ECS 实例，2核，Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz。

​	首先，受上一次实验的经验，为了尽可能提高效率，决定使用 C 语言的风格进行实验，实验过程中发现二维指针难以进行进程间数据传递，动态分配也不能分配出连续的二维数组，因此考虑使用一维数组来维护矩阵。

​	实验优化的方向主要是考虑如何并行优化矩阵乘法。可以将 $n\times n$ 的矩阵拆分成 $k$ 个 $x_i\times n$ 的矩阵（其中 $\sum x_i=n$，$k$ 为进程数量），实验为了方便，令 $k \mid n$ ，这样就可以让每个 $x_i$ 相等，即拆分成 $\frac{n}{k} \times n$ 的矩阵，这样每个进程计算复杂度为 $O(\frac{n^3}{k})$ 的矩阵乘法。理想情况下（不考虑数据传递，以及大量进程导致的系统负载），每个进程互不干预同时进行运算，理论时间复杂度为 $O(\frac{n^3}{k})$ 。

​	调试是编程非常重要的一环，实验过程中发现多进程程序难以使用 $gdb$ 调试，尤其是在没有图形化界面的服务器上面。考虑使用输出调试，但是发现并行会打乱输出顺序，因此需要使用锁来保护临界区。我想到两种办法，共享内存或者文件锁。这里我使用文件输出来进行调试。代码如下（其中 `LOCK_EX` 表示加互斥锁，`LOCK_UN` 表示解锁）。

```c++
#include <sys/file.h>
#include <stdio.h>
void print(context) {
    FILE *f = fopen("a.out", "a");
    int fd = fileno(f);
    flock(fd, LOCK_EX);
    fprintf(f,context); // print context
    ...					// other operations
    flock(fd, LOCK_UN);
}
```

​	由于文件读写必须以附加模式（`a mode`）打开。因此每次执行前需要清空输出文件。使用 shell 脚本可以简单实现。

```shell
cat /dev/null > a.out
mpicc test.c -o test
mpirun -n $n ./test
```



#### 1. **通过MPI实现通用矩阵乘法**

本实验计划使用 mpi 点对点通信实现矩阵乘法。

对 mpi 进行初始化后，由于每个进程都需要一个完整的矩阵变量。因此直接为矩阵分配空间。

```c++
int size, rank;
MPI_Init(NULL, NULL);
MPI_Comm_size(MPI_COMM_WORLD, &size);
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
float *mat = (float*) calloc(n * n, sizeof(float));
```

​	点对点通信中，一般是主进程（0号进程）读取数据，然后再将数据分发到其他进程上面去，最后再从其它进程上面接受结果。主进程代码如下。

```c++
if(rank == 0) {
    FILE *f = fopen("a.in", "r");
    if(f == NULL) {
        perror("error opening file");
        exit(-1);
    }
    for(int i = 0; i < n * n; ++ i)
        fscanf(f, "%f", mat + i);
    fclose(f);
    for(int i = 1; i < size; ++ i)
        MPI_Send(mat, n * n, MPI_FLOAT, i, 0, MPI_COMM_WORLD);
    for(int i = 1; i < size; ++ i) {
        int tot = n / (size - 1);
        if(i == size - 1) 
            tot = n - tot * (size - 2);
        MPI_Recv(mat + n / (size - 1) * (i - 1) * n, tot * n, MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }
}
```

​	其中主进程为其他进程分配 $\lfloor\frac{n}{size-1}\rfloor \times n$ 的矩阵，如果 $n \nmid (size-1)$ 那么最后一个进程会直接获取接下来的所有矩阵。之后调用 `MPI_Send` 将读到的矩阵发送到其他进程上面去。之后再使用 `MPI_Recv` 函数收取其他进程返回的结果。

​	**非主进程是核心执行部分**，主要是将得到的矩阵进行计算。下面的代码中，`solve` 函数是自己定义的矩阵乘法函数，下面会进行讲解。`tot` 变量是每一个当前进程分配到的矩阵行数，表示当前进程将处理 $tot \times N$ 大小的矩阵。$mat + n / (size-1)*(rank-1)*n$ 是当前矩阵的初始指针位置。

```c++
else {
    MPI_Recv(mat, n * n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    int tot = n / (size - 1);
    if(rank == size -1) 
        tot = n - tot * (size - 2);
    float *res = (float*) calloc(tot * n, sizeof(float));
    solve(mat + n / (size - 1) * (rank - 1) * n, mat, res, tot, n, n);
    MPI_Send(res, tot * n, MPI_FLOAT, 0, 0, MPI_COMM_WORLD);
    free(res);
}
```

​	然后是矩阵乘法 `solve` 函数的介绍，使用一维数组模拟矩阵。在 $N\times M$ 的矩阵中，第 $(i,j)$ 个元素对应一维数组上面的第 $i\times M + j$ 位元素。因此矩阵乘法实现如下。使用宏提前定义好可以在不影响代码效率的同时提高代码可读性。

```c++
// x is N * M , y is M * K and z is N * K
void solve(float *x, float *y, float *z, int N, int M, int K) {
    #define idx (i * M + k) 
    #define idy (k * K + j)
    #define idz (i * K + j)
    for(int i = 0; i < N; ++ i) 
        for(int j = 0; j < K; ++ j) {
            z[idz] = 0;
            for(int k = 0; k < M; ++ k) {
                z[idz] += x[idx] * y[idy];
            }
        }
    #undef idx
    #undef idy
    #undef idz
}
```



#### 2. **基于MPI的通用矩阵乘法优化**

##### 点对点通信

点对点通信上面已经实现。使用针对 `cache` 的优化，只需要优化 `solve` 函数。

```c++
// x is N * M , y is M * K and z is N * K
void solve(float *x, float *y, float *z, int N, int M, int K) {
    #define idx (i * M + k) 
    #define idy (k * K + j)
    #define idz (i * K + j)
    memset(z, 0, sizeof(float) * N * K);
    for(int i = 0; i < N; ++ i) {
        for(int k = 0; k < M; ++ k) {
            float tmp = x[idx];
            for(int j = 0; j < K; ++ j) {
                z[idz] += tmp * y[idy];
            }
        }
    }
    #undef idx
    #undef idy
    #undef idz
}
```

##### 聚合通信

​	类似于点对点通信的数据处理，让一个进程读取数据。但是使用聚合通信的函数。

​	首先需要使用 `MPI_Bcast` 函数将读取到的矩阵发放给其他所有进程。然后调用 `MPI_Scatter` 函数将矩阵等段分给各个进程（这里为了方便，令 $n \mid size$ ，其中 $n$ 是矩阵阶数得，如果 $n \nmid size$ ，那么可以提前让 $0$ 号进程（root 进程）处理好剩下的部分也就是倒数 $n \bmod size$ 行的矩阵 ，然后再进行 `MPI_Scatter` 和 `MPI_Gather` 聚合通信。

​	使用 `MPI_Scatter` 函数将矩阵等段分给各个进程后，调用已经实现的 `solve` 矩阵乘法函数。将结果存在 `res` 数组中，最后使用 `MPI_Gather` 函数将结果聚集，拼接到 $0$ 号进程 `mat` 指针上（此时已经计算完毕，`mat` 存储的原来的矩阵内容已经没有作用。核心代码实现如下。

```c++
MPI_Bcast(mat, n * n, MPI_FLOAT, 0, MPI_COMM_WORLD);
MPI_Scatter(mat, n / size * n, MPI_FLOAT, a, n / size * n, MPI_FLOAT, 0, MPI_COMM_WORLD);
solve(a, mat, res, n / size, n, n);
MPI_Gather(res, n * n / size, MPI_FLOAT, mat, n * n / size, MPI_FLOAT, 0, MPI_COMM_WORLD);
```

##### mpi_type_create_struct

​	进程间通信速度极慢，是性能瓶颈，反复传输小数据不如将小数据聚合起来一次性传输，也就是使用 `mpi_type_create_struct` 。但是，本实验中，进程间只进行了一次通信，使用 `mpi_type_create_struct` 也只是徒增负担，我认为完全不需要这个操作。

​	如果一定需要 `mpi_type_create_struct` 操作，我认为可以将原矩阵 $n\times n$ 矩阵，和拆分后的矩阵 $\lfloor\frac{n}{size}\rfloor \times n$ 的矩阵合并起来进行传送操作。



#### 3. 改造Lab1成矩阵乘法库函数

​	由于 Lab1 中大量使用模板，而模板的实例化在编译期间执行，因此实现必须和定义在一个文件中，否则会在链接过程中报链接错误。这样的写法无法编译为链接库，因此在这个版本中将 Lab1 的矩阵类改为以下定义。分别表示矩阵的数据，矩阵的行和列。为了提高效率，这里也使用了一维矩阵模拟二维矩阵的写法，二维矩阵的第 $i$ 行第 $j$ 列在一维矩阵中是第 $i\times col + j$ 位元素。

```c++
struct Matrix {
    std::vector<float> data;
    int row, col;
};
```

​	为了方便形如二维矩阵的取值，利用 `c++` 重载运算符的特性，重载括号运算符，目的是让 `Matrix(i,j)` 也能简单表示第 $i$ 行第 $j$ 列的数据。分别实现了 `const` 和非 `const` 版。

```c++
float &Matrix::operator () (const int i, const int j) {
    return data[i * col + j];
}
const float Matrix::operator () (const int i, const int j) const {
    return data[i * col + j];
}
```

​	为了提高效率，在构造函数中使用移动语义实现移动构造函数，赋值运算符同理。

```c++
Matrix::Matrix(Matrix &&rhs) {
    data.swap(rhs.data);
    std::swap(row,rhs.row);
    std::swap(col,rhs.col);
}
Matrix &Matrix::operator = (Matrix &&rhs) {
    row = rhs.row;
    col = rhs.col;
    data.swap(rhs.data);
    return *this;
}
```

​	除此之外，其他函数无变化。具体实现看代码。

目前我的函数全部定义在 `matrix.cpp` 文件中，编译动态库使用如下命令。

```shell
g++ matrix.cpp -shared -fPIC -o libmatrix.so
```

​	其中 `-shared` 表示建立动态库，而 `-fPIC` 表示位置无关代码(`Position-Independent Code`)，含义是产生的代码中没有绝对地址，全部使用相对地址。如果不加这个选项，那么代码段引用的数据需要重定位，会在链接时期产生这段代码的一份拷贝，而这不是动态链接库所期望的。

​	`libxxx.so` 是标准动态库的命名规则，编译器可以通过 `-l xxx` 来使用以该规则命名的动态库。



### 实验结果

#### 1. **通过MPI实现通用矩阵乘法**

​	似乎在某个版本后 mpirun 默认情况下无法使用 root 账户运行，需要加入参数 `--allow-run-as-root` ，同时 mpi 会估算 cpu 并行能力，如果并行程度超出了 mpi 认为的最高程度，就会禁止运行，使用 `--oversubscribe` 参数可以强制运行。

首先测试算法正确性，先随机生成一个 $4\times 4$ 的矩阵，通过随机数生成器，得到如下矩阵。

```
54 9 94 20 
31 84 88 95 
39 70 79 22 
16 70 61 74 
```

使用如下编译参数和运行指令（DEBUG用于控制输出）

```shell
mpicc mpiMatrix1.c -o test -DDEBUG
mpirun --allow-run-as-root --oversubscribe -n 3 ./test
```

输出结果是

```
7181.000000 9222.000000 14514.000000 5483.000000
9230.000000 20145.000000 23053.000000 17566.000000
7709.000000 13301.000000 17409.000000 10796.000000
6597.000000 15474.000000 16997.000000 13788.000000
```

​	经验证。结果正确

​	程序运行结束后，每个进程都会有一个自己的结束时间，这些时间往往不相同，这里使用了点对点通信，0 号进程是只负责消息传递的进程，最后所有进程的结果会传递到 0 号进程上，因此使用 0 号进程的结束时间来作为程序实际时间。

​	在实际操作中，会出现如下现象。

<img src="D:\课件\大三\高性能计算\lab\lab2\pic\wave.png" alt="wave" style="zoom:67%;" />

​	同样的操作，会出现极差为 $2.5s$ 的结果，猜想原因可能是因为进程接受消息的时间不同，虽然每个进程执行时间差不多，但是进程结束时间不一致。这样的误差是不能接受的，因此下列每项实验都是重复进行 $5$ 次取平均数的结果。

| 并行数量\矩阵大小 | 512      | 1024      | 2048       |
| ----------------- | -------- | --------- | ---------- |
| 2                 | 1.372491 | 13.201534 | 205.500078 |
| 3                 | 0.805690 | 10.614469 | 148.034331 |
| 4                 | 0.803894 | 11.624473 | 146.008191 |
| 5                 | 0.827373 | 9.385608  | 138.548964 |
| 6                 | 0.767554 | 9.532041  | 123.058667 |
| 7                 | 0.679792 | 9.707628  | 122.883527 |
| 8                 | 0.649875 | 10.373402 | 123.255619 |

​	观察实验结果，这个实验中采取了点对点通信实现矩阵乘法。可以发现并行数量从 $2 \rarr 3$ 的时候效率提升最高，在之后，执行时间也逐渐减少，但是变化不大。出现这样地变化是意料之中。因为用于计算的 CPU 是 $2$ 核，理论上可以让执行速度加倍。

​	为什么最后没有加倍呢，个人认为是 cache Miss 次数太多，太多数时间都去用于 DRAM 上寻址，由于朴素算法对 cache 不友好，在同一核上越来越多的进程只会让 cache 命中率进一步降低。而且每个进程都需要传输一个大小为 $n\times n$ 大小的矩阵，随着矩阵规模变大，传输的内存也会变得特别大。因此会有越来越多的时间用于消息传输上面，因此随着矩阵大小增大，并行数量越来越大，理论执行时间应该越大。

​	而实际表现是执行时间随着并行数量增大而减少，我认为出现这个现象的原因类似于 cpu 上的流水线，cpu 在系统负载高的情况下会分时执行，即每个进程会分出一个时间片执行，而计算机除了 mpi 并行执行的程序，还有很多其他服务进程（尤其是一直活跃的阿里云监控服务）。随着进程越来越多，一个周期内占有的执行时间比例越来越多，因此会逐渐变快。

#### 2. **基于MPI的通用矩阵乘法优化**

首先基于 cache 优化算法，`solve` 函数改为使用如下算法。

```c++
void solve(float *x, float *y, float *z, int N, int M, int K) {
    #define idx (i * M + k) 
    #define idy (k * K + j)
    #define idz (i * K + j)
    memset(z, 0, sizeof(float) * N * K);
    for(int i = 0; i < N; ++ i) {
        for(int k = 0; k < M; ++ k) {
            float tmp = x[idx];
            for(int j = 0; j < K; ++ j) {
                z[idz] += tmp * y[idy];
            }
        }
    }
    #undef idx
    #undef idy
    #undef idz
}
```

##### 点对点通信

验证算法正确性

仍然使用 $4\times 4$ 矩阵 

```
54 9 94 20 
31 84 88 95 
39 70 79 22 
16 70 61 74 
```

同样得到结果

```
7181.000000 9222.000000 14514.000000 5483.000000
9230.000000 20145.000000 23053.000000 17566.000000
7709.000000 13301.000000 17409.000000 10796.000000
6597.000000 15474.000000 16997.000000 13788.000000
```

经验证，结果正确。

| 并行数量\矩阵大小 | 512      | 1024     | 2048      |
| ----------------- | -------- | -------- | --------- |
| 2                 | 0.488491 | 3.619244 | 29.323683 |
| 3                 | 0.486172 | 3.076231 | 21.933362 |
| 4                 | 0.386841 | 2.968845 | 19.557340 |
| 5                 | 0.379242 | 2.542540 | 19.369420 |
| 6                 | 0.391096 | 2.643066 | 18.337636 |
| 7                 | 0.384582 | 2.594007 | 19.289445 |
| 8                 | 0.359710 | 2.609289 | 18.196723 |

​	这个结果有点出人意料，因为并行数量在 $2\rarr3$ 的时候最多也只减少了 $\frac{1}{3}$ 的时间。尤其是在矩阵规模是 $512\times 512$ 的时候，并行数量在 $2\rarr3$ 的时候几乎没有优化。我认为这里的瓶颈就在于上面提到的分时系统。

##### 聚合通信

验证算法正确性

仍然使用 $4\times 4$ 矩阵 

```
54 9 94 20 
31 84 88 95 
39 70 79 22 
16 70 61 74 
```

同样得到结果

```
7181.000000 9222.000000 14514.000000 5483.000000
9230.000000 20145.000000 23053.000000 17566.000000
7709.000000 13301.000000 17409.000000 10796.000000
6597.000000 15474.000000 16997.000000 13788.000000
```

经验证，结果正确。

| 并行数量\矩阵大小 | 512      | 1024     | 2048      |
| ----------------- | -------- | -------- | --------- |
| 1                 | 0.461785 | 3.746894 | 28.696034 |
| 2                 | 0.261383 | 1.996092 | 14.815303 |
| 3                 | 0.299890 | 2.066631 | 15.406910 |
| 4                 | 0.287675 | 2.005813 | 15.231905 |
| 5                 | 0.331011 | 2.172124 | 15.561654 |
| 6                 | 0.311499 | 2.352092 | 15.865979 |
| 7                 | 0.337086 | 2.211832 | 16.988813 |
| 8                 | 0.347188 | 2.274498 | 15.900999 |

​	这个的结果在预料之中。并行数量为 $1$ 的时候，即串行执行程序，比并行执行效率低一倍，而在并行程度为 $2$ 的基础上再次增加并行数量后，时间也没有显著提升。反而呈上升趋势。

​	似乎在聚合通信中，执行时间并没有受到太多分时系统的影响。猜想有可能是因为聚合通信提高了进程优先级。又或者聚合通信合理使用了共享内存，使用类似于 fork 写时复制的方式，只有当改变了值的时候才会真正复制一份拷贝，否则直接读取共享内存的数据。

#### 3. 改造Lab1成矩阵乘法库函数

通过 `g++ matrix.cpp -shared -fPIC -o libmatrix.so` 编译出动态库后。

对动态库进程测试。

在 `main.cpp` ，中，定义 $4\times 4$ 的矩阵 $A$， 通过计算得到矩阵 $A^2$ ，`main.cpp` 如下

```c++
#include "matrix.h"
int main() {
    Matrix x(4,4);
    x.data = std::vector<float>({1,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7});
    Matrix y(x);
    Matrix res = Strassen(x,y);
    res.print();
    res = normal(x,y);
    res.print();
}
```

编译参数如下，其中 `-L.` 表示动态库位置，`-lmatrix` 表示寻找 `libmatrix.so` 文件。

```shell
g++ main.cpp -o main -L. -lmatrix -DDEBUG
```

此时因为 `libmatrix.so` 文件不在系统路径中，因此此时仍然无法运行，需要将系统环境变量 `LD_LIBRARY_PATH` 暂时改为当前文件夹。

```shell
export LD_LIBRARY_PATH=.
./main
```

执行完毕后得到结果，两者结果均相同，且正确。（前四行是普通方法的结果，后四行是Strassen算法的结果）

```
54 37 47 57
130 93 119 145
44 41 56 71
111 79 101 123
54 37 47 57
130 93 119 145
44 41 56 71
111 79 101 123
```



### 实验感想

​	这次实验主要学习 mpi 的用法，整个实验过程中，尤其是点对点通信的情况下，总是要考虑数据传输的问题，这造成了大量的数据拷贝，浪费大量时间。我认为在一个机器上，更容易共享数据的多线程可能会有更高的效率。

​	但是为什么还要使用 mpi 呢，和同学讨论，以及在网上查阅资料发现，进程不仅仅局限于在单机上，如果要求多机并行，那么多线程就无能为力了。mpi 也提供了多机通信的接口。

